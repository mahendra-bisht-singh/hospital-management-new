
==> Audit <==
|-----------|-----------------|----------|--------------------------------|---------|---------------------|---------------------|
|  Command  |      Args       | Profile  |              User              | Version |     Start Time      |      End Time       |
|-----------|-----------------|----------|--------------------------------|---------|---------------------|---------------------|
| start     |                 | minikube | DESKTOP-GA1UUL7\Mahendra Singh | v1.36.0 | 25 Aug 25 14:02 IST |                     |
|           |                 |          | Bisht                          |         |                     |                     |
| start     | --driver=docker | minikube | DESKTOP-GA1UUL7\Mahendra Singh | v1.36.0 | 25 Aug 25 14:10 IST | 25 Aug 25 14:17 IST |
|           |                 |          | Bisht                          |         |                     |                     |
| dashboard |                 | minikube | DESKTOP-GA1UUL7\Mahendra Singh | v1.36.0 | 25 Aug 25 14:19 IST |                     |
|           |                 |          | Bisht                          |         |                     |                     |
| service   | hello-node      | minikube | DESKTOP-GA1UUL7\Mahendra Singh | v1.36.0 | 25 Aug 25 14:32 IST |                     |
|           |                 |          | Bisht                          |         |                     |                     |
|-----------|-----------------|----------|--------------------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/08/25 14:10:30
Running on machine: DESKTOP-GA1UUL7
Binary: Built with gc go1.24.0 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0825 14:10:30.133790   10304 out.go:345] Setting OutFile to fd 88 ...
I0825 14:10:30.135810   10304 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0825 14:10:30.135810   10304 out.go:358] Setting ErrFile to fd 92...
I0825 14:10:30.135810   10304 out.go:392] TERM=,COLORTERM=, which probably does not support color
W0825 14:10:30.149041   10304 root.go:314] Error reading config file at C:\Users\Mahendra Singh Bisht\.minikube\config\config.json: open C:\Users\Mahendra Singh Bisht\.minikube\config\config.json: The system cannot find the file specified.
I0825 14:10:30.189490   10304 out.go:352] Setting JSON to false
I0825 14:10:30.192843   10304 start.go:130] hostinfo: {"hostname":"DESKTOP-GA1UUL7","uptime":91868,"bootTime":1756019361,"procs":240,"os":"windows","platform":"Microsoft Windows 10 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.6216 Build 19045.6216","kernelVersion":"10.0.19045.6216 Build 19045.6216","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"11863f5e-ae1e-4b27-ad9f-a0e07efccb26"}
W0825 14:10:30.192937   10304 start.go:138] gopshost.Virtualization returned error: not implemented yet
I0825 14:10:30.365543   10304 out.go:177] * minikube v1.36.0 on Microsoft Windows 10 Home Single Language 10.0.19045.6216 Build 19045.6216
I0825 14:10:30.467081   10304 notify.go:220] Checking for updates...
I0825 14:10:30.501240   10304 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
E0825 14:10:30.728331   10304 start.go:819] api.Load failed for minikube: filestore "minikube": Docker machine "minikube" does not exist. Use "docker-machine ls" to list machines. Use "docker-machine create" to add a new one.
I0825 14:10:30.729386   10304 driver.go:404] Setting default libvirt URI to qemu:///system
E0825 14:10:30.731376   10304 start.go:819] api.Load failed for minikube: filestore "minikube": Docker machine "minikube" does not exist. Use "docker-machine ls" to list machines. Use "docker-machine create" to add a new one.
I0825 14:10:32.046991   10304 docker.go:123] docker version: linux-28.3.2:Docker Desktop 4.44.3 (202357)
I0825 14:10:32.064240   10304 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0825 14:10:37.435137   10304 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (5.3708694s)
I0825 14:10:37.435690   10304 info.go:266] docker info: {ID:7e1ba1f1-9c83-4b78-9adb-be45ede4d4ed Containers:2 ContainersRunning:2 ContainersPaused:0 ContainersStopped:0 Images:8 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:78 OomKillDisable:false NGoroutines:131 SystemTime:2025-08-25 08:40:35.331934718 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:3986919424 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.3.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.26.1-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.18] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.39.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.42] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.29] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.13.0] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.36] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.2]] Warnings:<nil>}}
I0825 14:10:37.720944   10304 out.go:177] * Using the docker driver based on existing profile
I0825 14:10:37.867145   10304 start.go:304] selected driver: docker
I0825 14:10:37.867145   10304 start.go:908] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Mahendra Singh Bisht:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0825 14:10:37.867145   10304 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0825 14:10:38.081723   10304 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0825 14:10:42.159921   10304 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (4.0781981s)
I0825 14:10:42.159921   10304 info.go:266] docker info: {ID:7e1ba1f1-9c83-4b78-9adb-be45ede4d4ed Containers:2 ContainersRunning:2 ContainersPaused:0 ContainersStopped:0 Images:8 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:78 OomKillDisable:false NGoroutines:131 SystemTime:2025-08-25 08:40:41.318834299 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:3986919424 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.3.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.26.1-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.18] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.39.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.42] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.29] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.13.0] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.36] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.2]] Warnings:<nil>}}
I0825 14:10:44.681511   10304 cni.go:84] Creating CNI manager for ""
I0825 14:10:44.681511   10304 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0825 14:10:44.681511   10304 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Mahendra Singh Bisht:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0825 14:10:44.777573   10304 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0825 14:10:44.891299   10304 cache.go:121] Beginning downloading kic base image for docker with docker
I0825 14:10:44.970909   10304 out.go:177] * Pulling base image v0.0.47 ...
I0825 14:10:45.106740   10304 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0825 14:10:45.107012   10304 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0825 14:10:45.108268   10304 preload.go:146] Found local preload: C:\Users\Mahendra Singh Bisht\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0825 14:10:45.108268   10304 cache.go:56] Caching tarball of preloaded images
I0825 14:10:45.109489   10304 preload.go:172] Found C:\Users\Mahendra Singh Bisht\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0825 14:10:45.109489   10304 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0825 14:10:45.110083   10304 profile.go:143] Saving config to C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\config.json ...
I0825 14:10:46.598729   10304 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b to local cache
I0825 14:10:46.599386   10304 localpath.go:146] windows sanitize: C:\Users\Mahendra Singh Bisht\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\Mahendra Singh Bisht\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0825 14:10:46.600337   10304 localpath.go:146] windows sanitize: C:\Users\Mahendra Singh Bisht\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\Mahendra Singh Bisht\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0825 14:10:46.600337   10304 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local cache directory
I0825 14:10:46.627883   10304 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local cache directory, skipping pull
I0825 14:10:46.627883   10304 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in cache, skipping pull
I0825 14:10:46.628278   10304 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b as a tarball
I0825 14:10:46.628278   10304 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from local cache
I0825 14:10:46.628278   10304 localpath.go:146] windows sanitize: C:\Users\Mahendra Singh Bisht\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\Mahendra Singh Bisht\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0825 14:12:55.121455   10304 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from cached tarball
I0825 14:12:55.141450   10304 cache.go:230] Successfully downloaded all kic artifacts
I0825 14:12:55.410765   10304 start.go:360] acquireMachinesLock for minikube: {Name:mk67ddc4933816e94fb72f5e0fdc83845910e115 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0825 14:12:55.411783   10304 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0825 14:12:55.547578   10304 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Mahendra Singh Bisht:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0825 14:12:55.547578   10304 start.go:125] createHost starting for "" (driver="docker")
I0825 14:12:55.759942   10304 out.go:235] * Creating docker container (CPUs=2, Memory=2200MB) ...
I0825 14:12:56.080684   10304 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0825 14:12:56.080684   10304 client.go:168] LocalClient.Create starting
I0825 14:12:56.133657   10304 main.go:141] libmachine: Creating CA: C:\Users\Mahendra Singh Bisht\.minikube\certs\ca.pem
I0825 14:12:56.703222   10304 main.go:141] libmachine: Creating client certificate: C:\Users\Mahendra Singh Bisht\.minikube\certs\cert.pem
I0825 14:13:00.840098   10304 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0825 14:13:01.880549   10304 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0825 14:13:01.880790   10304 cli_runner.go:217] Completed: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}": (1.0404515s)
I0825 14:13:01.885207   10304 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0825 14:13:01.885207   10304 cli_runner.go:164] Run: docker network inspect minikube
W0825 14:13:01.916264   10304 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0825 14:13:01.916264   10304 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0825 14:13:01.916264   10304 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0825 14:13:01.919879   10304 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0825 14:13:02.154808   10304 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0018b4120}
I0825 14:13:02.162920   10304 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0825 14:13:02.168336   10304 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0825 14:13:07.130352   10304 cli_runner.go:217] Completed: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube: (4.9620153s)
I0825 14:13:07.177475   10304 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0825 14:13:07.199051   10304 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0825 14:13:07.245630   10304 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0825 14:13:07.497886   10304 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0825 14:13:09.574171   10304 cli_runner.go:217] Completed: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true: (2.0759911s)
I0825 14:13:09.600261   10304 oci.go:103] Successfully created a docker volume minikube
I0825 14:13:09.622208   10304 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib
I0825 14:13:34.485764   10304 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib: (24.8635557s)
I0825 14:13:34.485764   10304 oci.go:107] Successfully prepared a docker volume minikube
I0825 14:13:34.485764   10304 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0825 14:13:34.485764   10304 kic.go:194] Starting extracting preloaded images to volume ...
I0825 14:13:34.507892   10304 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v "C:\Users\Mahendra Singh Bisht\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro" -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir
I0825 14:14:50.479089   10304 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v "C:\Users\Mahendra Singh Bisht\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro" -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir: (1m15.971197s)
I0825 14:14:50.479089   10304 kic.go:203] duration metric: took 1m15.993325s to extract preloaded images to volume ...
I0825 14:14:50.574212   10304 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0825 14:15:03.473293   10304 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (12.8990413s)
I0825 14:15:03.731208   10304 info.go:266] docker info: {ID:7e1ba1f1-9c83-4b78-9adb-be45ede4d4ed Containers:2 ContainersRunning:2 ContainersPaused:0 ContainersStopped:0 Images:10 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:80 OomKillDisable:false NGoroutines:135 SystemTime:2025-08-25 08:45:02.869520755 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:3986919424 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.3.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.26.1-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.18] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.39.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.42] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.29] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.13.0] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.36] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.2]] Warnings:<nil>}}
I0825 14:15:03.807069   10304 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0825 14:15:06.473035   10304 cli_runner.go:217] Completed: docker info --format "'{{json .SecurityOptions}}'": (2.6659658s)
I0825 14:15:06.511769   10304 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b
I0825 14:15:17.770249   10304 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b: (11.2584797s)
I0825 14:15:17.776931   10304 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0825 14:15:18.058190   10304 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0825 14:15:18.136081   10304 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0825 14:15:20.036426   10304 cli_runner.go:217] Completed: docker exec minikube stat /var/lib/dpkg/alternatives/iptables: (1.9003446s)
I0825 14:15:20.144091   10304 oci.go:144] the created container "minikube" has a running status.
I0825 14:15:20.175045   10304 kic.go:225] Creating ssh key for kic: C:\Users\Mahendra Singh Bisht\.minikube\machines\minikube\id_rsa...
I0825 14:15:21.693439   10304 kic_runner.go:191] docker (temp): C:\Users\Mahendra Singh Bisht\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0825 14:15:23.352497   10304 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0825 14:15:23.711080   10304 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0825 14:15:23.711080   10304 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0825 14:15:24.780013   10304 kic_runner.go:123] Done: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]: (1.068933s)
I0825 14:15:24.788617   10304 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\Mahendra Singh Bisht\.minikube\machines\minikube\id_rsa...
W0825 14:15:32.814127   10304 kic.go:271] unable to determine current user's SID. minikube tunnel may not work.
I0825 14:15:32.834992   10304 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0825 14:15:32.886135   10304 machine.go:93] provisionDockerMachine start ...
I0825 14:15:32.900875   10304 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 14:15:33.241342   10304 main.go:141] libmachine: Using SSH client type: native
I0825 14:15:40.785247   10304 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xbda9e0] 0xbdd520 <nil>  [] 0s} 127.0.0.1 59273 <nil> <nil>}
I0825 14:15:40.785247   10304 main.go:141] libmachine: About to run SSH command:
hostname
I0825 14:15:42.100206   10304 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0825 14:15:42.142366   10304 ubuntu.go:169] provisioning hostname "minikube"
I0825 14:15:42.216770   10304 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 14:15:42.429595   10304 main.go:141] libmachine: Using SSH client type: native
I0825 14:15:42.429595   10304 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xbda9e0] 0xbdd520 <nil>  [] 0s} 127.0.0.1 59273 <nil> <nil>}
I0825 14:15:42.429595   10304 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0825 14:15:42.903999   10304 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0825 14:15:42.944085   10304 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 14:15:43.135273   10304 main.go:141] libmachine: Using SSH client type: native
I0825 14:15:43.135273   10304 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xbda9e0] 0xbdd520 <nil>  [] 0s} 127.0.0.1 59273 <nil> <nil>}
I0825 14:15:43.135273   10304 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0825 14:15:43.394449   10304 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0825 14:15:43.429920   10304 ubuntu.go:175] set auth options {CertDir:C:\Users\Mahendra Singh Bisht\.minikube CaCertPath:C:\Users\Mahendra Singh Bisht\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Mahendra Singh Bisht\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Mahendra Singh Bisht\.minikube\machines\server.pem ServerKeyPath:C:\Users\Mahendra Singh Bisht\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Mahendra Singh Bisht\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Mahendra Singh Bisht\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Mahendra Singh Bisht\.minikube}
I0825 14:15:43.429920   10304 ubuntu.go:177] setting up certificates
I0825 14:15:43.429920   10304 provision.go:84] configureAuth start
I0825 14:15:43.433013   10304 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0825 14:15:43.541485   10304 provision.go:143] copyHostCerts
I0825 14:15:43.576225   10304 exec_runner.go:151] cp: C:\Users\Mahendra Singh Bisht\.minikube\certs\ca.pem --> C:\Users\Mahendra Singh Bisht\.minikube/ca.pem (1115 bytes)
I0825 14:15:43.610711   10304 exec_runner.go:151] cp: C:\Users\Mahendra Singh Bisht\.minikube\certs\cert.pem --> C:\Users\Mahendra Singh Bisht\.minikube/cert.pem (1159 bytes)
I0825 14:15:43.646068   10304 exec_runner.go:151] cp: C:\Users\Mahendra Singh Bisht\.minikube\certs\key.pem --> C:\Users\Mahendra Singh Bisht\.minikube/key.pem (1679 bytes)
I0825 14:15:43.699015   10304 provision.go:117] generating server cert: C:\Users\Mahendra Singh Bisht\.minikube\machines\server.pem ca-key=C:\Users\Mahendra Singh Bisht\.minikube\certs\ca.pem private-key=C:\Users\Mahendra Singh Bisht\.minikube\certs\ca-key.pem org=Mahendra Singh Bisht.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0825 14:15:44.205806   10304 provision.go:177] copyRemoteCerts
I0825 14:15:44.225311   10304 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0825 14:15:44.230132   10304 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 14:15:44.287874   10304 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59273 SSHKeyPath:C:\Users\Mahendra Singh Bisht\.minikube\machines\minikube\id_rsa Username:docker}
I0825 14:15:44.438525   10304 ssh_runner.go:362] scp C:\Users\Mahendra Singh Bisht\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1115 bytes)
I0825 14:15:44.571948   10304 ssh_runner.go:362] scp C:\Users\Mahendra Singh Bisht\.minikube\machines\server.pem --> /etc/docker/server.pem (1216 bytes)
I0825 14:15:44.972761   10304 ssh_runner.go:362] scp C:\Users\Mahendra Singh Bisht\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0825 14:15:45.075583   10304 provision.go:87] duration metric: took 1.6363284s to configureAuth
I0825 14:15:45.075583   10304 ubuntu.go:193] setting minikube options for container-runtime
I0825 14:15:45.103071   10304 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0825 14:15:45.107403   10304 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 14:15:45.143843   10304 main.go:141] libmachine: Using SSH client type: native
I0825 14:15:45.143843   10304 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xbda9e0] 0xbdd520 <nil>  [] 0s} 127.0.0.1 59273 <nil> <nil>}
I0825 14:15:45.143843   10304 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0825 14:15:45.437517   10304 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0825 14:15:45.437517   10304 ubuntu.go:71] root file system type: overlay
I0825 14:15:45.517027   10304 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0825 14:15:45.527774   10304 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 14:15:45.609908   10304 main.go:141] libmachine: Using SSH client type: native
I0825 14:15:45.609908   10304 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xbda9e0] 0xbdd520 <nil>  [] 0s} 127.0.0.1 59273 <nil> <nil>}
I0825 14:15:45.609908   10304 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0825 14:15:45.833261   10304 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0825 14:15:45.866738   10304 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 14:15:46.059890   10304 main.go:141] libmachine: Using SSH client type: native
I0825 14:15:46.060488   10304 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xbda9e0] 0xbdd520 <nil>  [] 0s} 127.0.0.1 59273 <nil> <nil>}
I0825 14:15:46.060488   10304 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0825 14:15:50.300767   10304 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-04-18 09:50:48.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-08-25 08:45:45.831210589 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0825 14:15:50.300767   10304 machine.go:96] duration metric: took 17.414632s to provisionDockerMachine
I0825 14:15:50.301685   10304 client.go:171] duration metric: took 2m54.2210003s to LocalClient.Create
I0825 14:15:50.301761   10304 start.go:167] duration metric: took 2m54.2210358s to libmachine.API.Create "minikube"
I0825 14:15:50.430164   10304 start.go:293] postStartSetup for "minikube" (driver="docker")
I0825 14:15:50.430164   10304 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0825 14:15:50.464171   10304 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0825 14:15:50.475067   10304 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 14:15:50.532503   10304 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59273 SSHKeyPath:C:\Users\Mahendra Singh Bisht\.minikube\machines\minikube\id_rsa Username:docker}
I0825 14:15:50.856315   10304 ssh_runner.go:195] Run: cat /etc/os-release
I0825 14:15:50.908321   10304 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0825 14:15:50.908321   10304 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0825 14:15:50.908321   10304 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0825 14:15:50.908321   10304 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0825 14:15:50.941362   10304 filesync.go:126] Scanning C:\Users\Mahendra Singh Bisht\.minikube\addons for local assets ...
I0825 14:15:51.056788   10304 filesync.go:126] Scanning C:\Users\Mahendra Singh Bisht\.minikube\files for local assets ...
I0825 14:15:51.056788   10304 start.go:296] duration metric: took 626.624ms for postStartSetup
I0825 14:15:51.094205   10304 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0825 14:15:51.163747   10304 profile.go:143] Saving config to C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\config.json ...
I0825 14:15:51.322876   10304 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0825 14:15:51.331447   10304 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 14:15:51.527329   10304 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59273 SSHKeyPath:C:\Users\Mahendra Singh Bisht\.minikube\machines\minikube\id_rsa Username:docker}
I0825 14:15:51.637472   10304 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0825 14:15:51.668391   10304 start.go:128] duration metric: took 2m56.0951846s to createHost
I0825 14:15:51.668391   10304 start.go:83] releasing machines lock for "minikube", held for 2m56.2566084s
I0825 14:15:51.695929   10304 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0825 14:15:51.750265   10304 ssh_runner.go:195] Run: cat /version.json
I0825 14:15:51.753365   10304 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 14:15:51.758117   10304 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0825 14:15:51.776170   10304 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 14:15:51.793266   10304 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59273 SSHKeyPath:C:\Users\Mahendra Singh Bisht\.minikube\machines\minikube\id_rsa Username:docker}
I0825 14:15:51.816994   10304 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59273 SSHKeyPath:C:\Users\Mahendra Singh Bisht\.minikube\machines\minikube\id_rsa Username:docker}
I0825 14:15:51.954051   10304 ssh_runner.go:195] Run: systemctl --version
W0825 14:15:51.959775   10304 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0825 14:15:51.966174   10304 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0825 14:15:51.983079   10304 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0825 14:15:51.993189   10304 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0825 14:15:52.009544   10304 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0825 14:15:52.469555   10304 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0825 14:15:52.469555   10304 start.go:495] detecting cgroup driver to use...
I0825 14:15:52.469555   10304 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0825 14:15:52.543706   10304 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0825 14:15:52.701483   10304 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0825 14:15:52.752330   10304 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0825 14:15:52.765302   10304 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0825 14:15:52.769484   10304 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0825 14:15:52.785037   10304 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0825 14:15:52.811890   10304 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0825 14:15:52.845217   10304 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0825 14:15:52.861573   10304 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0825 14:15:52.876139   10304 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0825 14:15:52.891577   10304 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0825 14:15:53.017208   10304 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0825 14:15:53.060210   10304 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0825 14:15:53.077062   10304 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0825 14:15:53.115360   10304 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0825 14:15:53.250694   10304 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0825 14:15:53.738830   10304 start.go:495] detecting cgroup driver to use...
I0825 14:15:53.738830   10304 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0825 14:15:53.747681   10304 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0825 14:15:53.783238   10304 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0825 14:15:53.789772   10304 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0825 14:15:53.804552   10304 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0825 14:15:53.831450   10304 ssh_runner.go:195] Run: which cri-dockerd
I0825 14:15:53.858979   10304 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0825 14:15:53.871095   10304 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0825 14:15:53.896904   10304 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0825 14:15:54.353407   10304 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0825 14:15:54.487244   10304 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0825 14:15:54.534548   10304 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0825 14:15:54.562047   10304 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0825 14:15:54.580439   10304 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0825 14:15:54.663355   10304 ssh_runner.go:195] Run: sudo systemctl restart docker
W0825 14:15:55.098281   10304 out.go:270] ! Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0825 14:15:55.099057   10304 out.go:270] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0825 14:16:00.355397   10304 ssh_runner.go:235] Completed: sudo systemctl restart docker: (5.6920424s)
I0825 14:16:00.361621   10304 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0825 14:16:00.381305   10304 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0825 14:16:00.401431   10304 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0825 14:16:00.521348   10304 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0825 14:16:00.605198   10304 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0825 14:16:00.675427   10304 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0825 14:16:00.736273   10304 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0825 14:16:00.756984   10304 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0825 14:16:00.893197   10304 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0825 14:16:01.446424   10304 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0825 14:16:01.461448   10304 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0825 14:16:01.474101   10304 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0825 14:16:01.479656   10304 start.go:563] Will wait 60s for crictl version
I0825 14:16:01.486486   10304 ssh_runner.go:195] Run: which crictl
I0825 14:16:01.500467   10304 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0825 14:16:01.582407   10304 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0825 14:16:01.586405   10304 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0825 14:16:01.638599   10304 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0825 14:16:01.746345   10304 out.go:235] * Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0825 14:16:01.750037   10304 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0825 14:16:02.463555   10304 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0825 14:16:02.615391   10304 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0825 14:16:02.631071   10304 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0825 14:16:02.665040   10304 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0825 14:16:02.960700   10304 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Mahendra Singh Bisht:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0825 14:16:03.017631   10304 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0825 14:16:03.026814   10304 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0825 14:16:03.075566   10304 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0825 14:16:03.075566   10304 docker.go:632] Images already preloaded, skipping extraction
I0825 14:16:03.164009   10304 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0825 14:16:03.187591   10304 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0825 14:16:03.187591   10304 cache_images.go:84] Images are preloaded, skipping loading
I0825 14:16:03.187591   10304 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.33.1 docker true true} ...
I0825 14:16:03.290655   10304 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0825 14:16:03.295378   10304 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0825 14:16:03.487319   10304 cni.go:84] Creating CNI manager for ""
I0825 14:16:03.487319   10304 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0825 14:16:03.532631   10304 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0825 14:16:03.532631   10304 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0825 14:16:03.532631   10304 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0825 14:16:03.539578   10304 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0825 14:16:03.551992   10304 binaries.go:44] Found k8s binaries, skipping transfer
I0825 14:16:03.558477   10304 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0825 14:16:03.583403   10304 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0825 14:16:03.602951   10304 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0825 14:16:03.623041   10304 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0825 14:16:03.649166   10304 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0825 14:16:03.653961   10304 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0825 14:16:03.672499   10304 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0825 14:16:03.887766   10304 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0825 14:16:04.105348   10304 certs.go:68] Setting up C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube for IP: 192.168.49.2
I0825 14:16:04.177637   10304 certs.go:194] generating shared ca certs ...
I0825 14:16:04.177637   10304 certs.go:226] acquiring lock for ca certs: {Name:mk34e33e3b513930f66cf9aea80ed42d0c034762 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0825 14:16:04.177637   10304 certs.go:240] generating "minikubeCA" ca cert: C:\Users\Mahendra Singh Bisht\.minikube\ca.key
I0825 14:16:04.708190   10304 crypto.go:156] Writing cert to C:\Users\Mahendra Singh Bisht\.minikube\ca.crt ...
I0825 14:16:04.708190   10304 lock.go:35] WriteFile acquiring C:\Users\Mahendra Singh Bisht\.minikube\ca.crt: {Name:mkc967a4d51034593835ae17f88283c00aced507 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0825 14:16:04.727436   10304 crypto.go:164] Writing key to C:\Users\Mahendra Singh Bisht\.minikube\ca.key ...
I0825 14:16:04.727436   10304 lock.go:35] WriteFile acquiring C:\Users\Mahendra Singh Bisht\.minikube\ca.key: {Name:mk9f5342d002ac8abbc4d013c25bbc7241c2977a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0825 14:16:04.799503   10304 certs.go:240] generating "proxyClientCA" ca cert: C:\Users\Mahendra Singh Bisht\.minikube\proxy-client-ca.key
I0825 14:16:04.881218   10304 crypto.go:156] Writing cert to C:\Users\Mahendra Singh Bisht\.minikube\proxy-client-ca.crt ...
I0825 14:16:04.882223   10304 lock.go:35] WriteFile acquiring C:\Users\Mahendra Singh Bisht\.minikube\proxy-client-ca.crt: {Name:mk8fb3d51ba0dcfd33593a092e4b72d9bf6eb7eb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0825 14:16:04.944899   10304 crypto.go:164] Writing key to C:\Users\Mahendra Singh Bisht\.minikube\proxy-client-ca.key ...
I0825 14:16:04.944899   10304 lock.go:35] WriteFile acquiring C:\Users\Mahendra Singh Bisht\.minikube\proxy-client-ca.key: {Name:mk235b96e20bce4538e85e90d87f5e1fb253aa2f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0825 14:16:05.000701   10304 certs.go:256] generating profile certs ...
I0825 14:16:05.000701   10304 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\client.key
I0825 14:16:05.024298   10304 crypto.go:68] Generating cert C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\client.crt with IP's: []
I0825 14:16:05.906893   10304 crypto.go:156] Writing cert to C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\client.crt ...
I0825 14:16:05.906893   10304 lock.go:35] WriteFile acquiring C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\client.crt: {Name:mk946c7d26c583b805adb23ce94a9fff5998a398 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0825 14:16:05.907898   10304 crypto.go:164] Writing key to C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\client.key ...
I0825 14:16:05.907898   10304 lock.go:35] WriteFile acquiring C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\client.key: {Name:mk1df0c66684d74fdc43f8bda1036a26f18987f0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0825 14:16:06.004077   10304 certs.go:363] generating signed profile cert for "minikube": C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0825 14:16:06.004738   10304 crypto.go:68] Generating cert C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0825 14:16:06.122200   10304 crypto.go:156] Writing cert to C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I0825 14:16:06.122200   10304 lock.go:35] WriteFile acquiring C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mkfc75634e5dab0c83241b5d62b0ce05b4321c68 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0825 14:16:06.138044   10304 crypto.go:164] Writing key to C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I0825 14:16:06.138044   10304 lock.go:35] WriteFile acquiring C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mk6dd8cd6a4369aa8816457af22ad96060d6a011 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0825 14:16:06.139037   10304 certs.go:381] copying C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\apiserver.crt
I0825 14:16:06.296720   10304 certs.go:385] copying C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\apiserver.key
I0825 14:16:06.464201   10304 certs.go:363] generating signed profile cert for "aggregator": C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\proxy-client.key
I0825 14:16:06.464720   10304 crypto.go:68] Generating cert C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0825 14:16:06.982269   10304 crypto.go:156] Writing cert to C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\proxy-client.crt ...
I0825 14:16:06.982269   10304 lock.go:35] WriteFile acquiring C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\proxy-client.crt: {Name:mk620f923682516e752f78551d7571096c032bef Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0825 14:16:06.987234   10304 crypto.go:164] Writing key to C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\proxy-client.key ...
I0825 14:16:06.987234   10304 lock.go:35] WriteFile acquiring C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\proxy-client.key: {Name:mkba919103644651b6b28dbcc218b4d21d2289bb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0825 14:16:07.242345   10304 certs.go:484] found cert: C:\Users\Mahendra Singh Bisht\.minikube\certs\ca-key.pem (1675 bytes)
I0825 14:16:07.266294   10304 certs.go:484] found cert: C:\Users\Mahendra Singh Bisht\.minikube\certs\ca.pem (1115 bytes)
I0825 14:16:07.276700   10304 certs.go:484] found cert: C:\Users\Mahendra Singh Bisht\.minikube\certs\cert.pem (1159 bytes)
I0825 14:16:07.288846   10304 certs.go:484] found cert: C:\Users\Mahendra Singh Bisht\.minikube\certs\key.pem (1679 bytes)
I0825 14:16:07.708812   10304 ssh_runner.go:362] scp C:\Users\Mahendra Singh Bisht\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0825 14:16:07.770219   10304 ssh_runner.go:362] scp C:\Users\Mahendra Singh Bisht\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0825 14:16:07.798577   10304 ssh_runner.go:362] scp C:\Users\Mahendra Singh Bisht\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0825 14:16:07.825008   10304 ssh_runner.go:362] scp C:\Users\Mahendra Singh Bisht\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0825 14:16:07.854423   10304 ssh_runner.go:362] scp C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0825 14:16:07.886435   10304 ssh_runner.go:362] scp C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0825 14:16:07.914781   10304 ssh_runner.go:362] scp C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0825 14:16:07.958188   10304 ssh_runner.go:362] scp C:\Users\Mahendra Singh Bisht\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0825 14:16:07.986108   10304 ssh_runner.go:362] scp C:\Users\Mahendra Singh Bisht\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0825 14:16:08.013705   10304 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0825 14:16:08.141918   10304 ssh_runner.go:195] Run: openssl version
I0825 14:16:08.155765   10304 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0825 14:16:08.193509   10304 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0825 14:16:08.199214   10304 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Aug 25 08:46 /usr/share/ca-certificates/minikubeCA.pem
I0825 14:16:08.206112   10304 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0825 14:16:08.219918   10304 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0825 14:16:08.237311   10304 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0825 14:16:08.243389   10304 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0825 14:16:08.243389   10304 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Mahendra Singh Bisht:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0825 14:16:08.246546   10304 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0825 14:16:08.274984   10304 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0825 14:16:08.291808   10304 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0825 14:16:08.331620   10304 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0825 14:16:08.337841   10304 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0825 14:16:08.399369   10304 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0825 14:16:08.399369   10304 kubeadm.go:157] found existing configuration files:

I0825 14:16:08.464828   10304 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0825 14:16:08.482306   10304 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0825 14:16:08.488540   10304 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0825 14:16:08.504769   10304 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0825 14:16:08.515281   10304 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0825 14:16:08.521501   10304 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0825 14:16:08.539606   10304 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0825 14:16:08.549875   10304 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0825 14:16:08.556060   10304 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0825 14:16:08.572284   10304 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0825 14:16:08.582122   10304 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0825 14:16:08.588370   10304 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0825 14:16:08.701459   10304 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0825 14:16:11.710029   10304 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0825 14:16:11.897176   10304 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0825 14:17:03.019105   10304 kubeadm.go:310] [init] Using Kubernetes version: v1.33.1
I0825 14:17:03.019105   10304 kubeadm.go:310] [preflight] Running pre-flight checks
I0825 14:17:03.019105   10304 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0825 14:17:03.019105   10304 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0825 14:17:03.019105   10304 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0825 14:17:03.019105   10304 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0825 14:17:03.058474   10304 out.go:235]   - Generating certificates and keys ...
I0825 14:17:03.059284   10304 kubeadm.go:310] [certs] Using existing ca certificate authority
I0825 14:17:03.059284   10304 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0825 14:17:03.059284   10304 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0825 14:17:03.059284   10304 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0825 14:17:03.059284   10304 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0825 14:17:03.059284   10304 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0825 14:17:03.059284   10304 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0825 14:17:03.059801   10304 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0825 14:17:03.059801   10304 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0825 14:17:03.059801   10304 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0825 14:17:03.059801   10304 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0825 14:17:03.059801   10304 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0825 14:17:03.059801   10304 kubeadm.go:310] [certs] Generating "sa" key and public key
I0825 14:17:03.059801   10304 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0825 14:17:03.059801   10304 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0825 14:17:03.060320   10304 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0825 14:17:03.060320   10304 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0825 14:17:03.060320   10304 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0825 14:17:03.060320   10304 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0825 14:17:03.060320   10304 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0825 14:17:03.060320   10304 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0825 14:17:03.107319   10304 out.go:235]   - Booting up control plane ...
I0825 14:17:03.109854   10304 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0825 14:17:03.109854   10304 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0825 14:17:03.110483   10304 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0825 14:17:03.111009   10304 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0825 14:17:03.111084   10304 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0825 14:17:03.111612   10304 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0825 14:17:03.112510   10304 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0825 14:17:03.113099   10304 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0825 14:17:03.113099   10304 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 2.039583243s
I0825 14:17:03.113681   10304 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0825 14:17:03.113681   10304 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I0825 14:17:03.114270   10304 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0825 14:17:03.114270   10304 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0825 14:17:03.114819   10304 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 10.830531367s
I0825 14:17:03.114819   10304 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 13.799145411s
I0825 14:17:03.115403   10304 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 36.001600084s
I0825 14:17:03.115403   10304 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0825 14:17:03.116046   10304 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0825 14:17:03.116046   10304 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0825 14:17:03.116757   10304 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0825 14:17:03.117336   10304 kubeadm.go:310] [bootstrap-token] Using token: vzhaf5.27l9viptwp0q4lt0
I0825 14:17:03.157012   10304 out.go:235]   - Configuring RBAC rules ...
I0825 14:17:03.159457   10304 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0825 14:17:03.160067   10304 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0825 14:17:03.160067   10304 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0825 14:17:03.160627   10304 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0825 14:17:03.161370   10304 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0825 14:17:03.161983   10304 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0825 14:17:03.162127   10304 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0825 14:17:03.162127   10304 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0825 14:17:03.162681   10304 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0825 14:17:03.162681   10304 kubeadm.go:310] 
I0825 14:17:03.162681   10304 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0825 14:17:03.162681   10304 kubeadm.go:310] 
I0825 14:17:03.163279   10304 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0825 14:17:03.163279   10304 kubeadm.go:310] 
I0825 14:17:03.163279   10304 kubeadm.go:310]   mkdir -p $HOME/.kube
I0825 14:17:03.163279   10304 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0825 14:17:03.163851   10304 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0825 14:17:03.163851   10304 kubeadm.go:310] 
I0825 14:17:03.163851   10304 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0825 14:17:03.163851   10304 kubeadm.go:310] 
I0825 14:17:03.164409   10304 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0825 14:17:03.164409   10304 kubeadm.go:310] 
I0825 14:17:03.164409   10304 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0825 14:17:03.164943   10304 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0825 14:17:03.164962   10304 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0825 14:17:03.164962   10304 kubeadm.go:310] 
I0825 14:17:03.165540   10304 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0825 14:17:03.165540   10304 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0825 14:17:03.165540   10304 kubeadm.go:310] 
I0825 14:17:03.166108   10304 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token vzhaf5.27l9viptwp0q4lt0 \
I0825 14:17:03.166656   10304 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:cfbff915ff8ffc89d8739a85fcb56fee081ead55e4c733c352791b3614b477c7 \
I0825 14:17:03.166656   10304 kubeadm.go:310] 	--control-plane 
I0825 14:17:03.166656   10304 kubeadm.go:310] 
I0825 14:17:03.166656   10304 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0825 14:17:03.166656   10304 kubeadm.go:310] 
I0825 14:17:03.167214   10304 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token vzhaf5.27l9viptwp0q4lt0 \
I0825 14:17:03.167745   10304 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:cfbff915ff8ffc89d8739a85fcb56fee081ead55e4c733c352791b3614b477c7 
I0825 14:17:03.167745   10304 cni.go:84] Creating CNI manager for ""
I0825 14:17:03.167745   10304 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0825 14:17:03.206942   10304 out.go:177] * Configuring bridge CNI (Container Networking Interface) ...
I0825 14:17:03.277758   10304 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0825 14:17:03.343227   10304 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0825 14:17:03.411266   10304 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0825 14:17:03.421843   10304 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0825 14:17:03.478344   10304 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_08_25T14_17_03_0700 minikube.k8s.io/version=v1.36.0 minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0825 14:17:12.837937   10304 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_08_25T14_17_03_0700 minikube.k8s.io/version=v1.36.0 minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true: (9.3595925s)
I0825 14:17:12.837937   10304 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.33.1/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig: (9.4160939s)
I0825 14:17:12.837937   10304 kubeadm.go:1105] duration metric: took 9.4266704s to wait for elevateKubeSystemPrivileges
I0825 14:17:12.837937   10304 ssh_runner.go:235] Completed: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj": (9.4266704s)
I0825 14:17:12.837937   10304 ops.go:34] apiserver oom_adj: -16
I0825 14:17:12.837937   10304 kubeadm.go:394] duration metric: took 1m4.5945477s to StartCluster
I0825 14:17:12.837937   10304 settings.go:142] acquiring lock: {Name:mkace34fe1438ba20e62f9b41fdfccb0142cb813 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0825 14:17:12.837937   10304 settings.go:150] Updating kubeconfig:  C:\Users\Mahendra Singh Bisht\.kube\config
I0825 14:17:12.838473   10304 lock.go:35] WriteFile acquiring C:\Users\Mahendra Singh Bisht\.kube\config: {Name:mk26bf17a81ad0d3bca87151e01f2acd86f838b5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0825 14:17:12.839613   10304 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0825 14:17:12.977411   10304 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0825 14:17:13.026512   10304 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0825 14:17:13.095333   10304 out.go:177] * Verifying Kubernetes components...
I0825 14:17:13.184163   10304 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0825 14:17:13.049713   10304 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0825 14:17:13.185196   10304 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0825 14:17:13.185196   10304 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0825 14:17:13.222442   10304 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0825 14:17:13.228441   10304 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0825 14:17:13.241230   10304 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0825 14:17:13.389131   10304 host.go:66] Checking if "minikube" exists ...
I0825 14:17:13.614887   10304 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0825 14:17:13.614887   10304 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0825 14:17:13.783870   10304 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0825 14:17:13.841900   10304 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0825 14:17:13.841900   10304 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0825 14:17:13.853471   10304 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 14:17:13.915061   10304 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59273 SSHKeyPath:C:\Users\Mahendra Singh Bisht\.minikube\machines\minikube\id_rsa Username:docker}
I0825 14:17:13.969884   10304 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0825 14:17:14.004294   10304 start.go:971] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0825 14:17:14.028231   10304 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0825 14:17:14.520088   10304 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0825 14:17:16.937478   10304 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0825 14:17:16.938828   10304 host.go:66] Checking if "minikube" exists ...
I0825 14:17:16.940086   10304 api_server.go:52] waiting for apiserver process to appear ...
I0825 14:17:16.967581   10304 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0825 14:17:16.968175   10304 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0825 14:17:17.176028   10304 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0825 14:17:17.176028   10304 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0825 14:17:17.182989   10304 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 14:17:17.417618   10304 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59273 SSHKeyPath:C:\Users\Mahendra Singh Bisht\.minikube\machines\minikube\id_rsa Username:docker}
I0825 14:17:17.556679   10304 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0825 14:17:18.819729   10304 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (4.2996404s)
I0825 14:17:18.819729   10304 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.8521477s)
I0825 14:17:18.819729   10304 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.2630342s)
I0825 14:17:18.819729   10304 api_server.go:72] duration metric: took 5.8423179s to wait for apiserver process to appear ...
I0825 14:17:18.819729   10304 api_server.go:88] waiting for apiserver healthz status ...
I0825 14:17:18.907423   10304 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59277/healthz ...
I0825 14:17:18.989165   10304 api_server.go:279] https://127.0.0.1:59277/healthz returned 200:
ok
I0825 14:17:19.160803   10304 api_server.go:141] control plane version: v1.33.1
I0825 14:17:19.160803   10304 api_server.go:131] duration metric: took 341.0746ms to wait for apiserver health ...
I0825 14:17:19.160803   10304 system_pods.go:43] waiting for kube-system pods to appear ...
I0825 14:17:19.782457   10304 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0825 14:17:20.583731   10304 out.go:177] * Enabled addons: storage-provisioner, default-storageclass
I0825 14:17:20.666337   10304 addons.go:514] duration metric: took 7.720102s for enable addons: enabled=[storage-provisioner default-storageclass]
I0825 14:17:20.677744   10304 system_pods.go:59] 8 kube-system pods found
I0825 14:17:20.677744   10304 system_pods.go:61] "coredns-674b8bbfcf-ksw6k" [259d1acb-decf-47ca-852e-61d5aecaaf53] Pending / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0825 14:17:20.677744   10304 system_pods.go:61] "coredns-674b8bbfcf-x6hpz" [a698c6ae-9dda-42a7-adeb-6de80a76913b] Pending / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0825 14:17:20.677744   10304 system_pods.go:61] "etcd-minikube" [cd5cd2eb-1d17-4b93-b88a-3d30a8812d5c] Running
I0825 14:17:20.677744   10304 system_pods.go:61] "kube-apiserver-minikube" [644fbc3d-86b3-4d5e-82d6-cf29233d1e9c] Running
I0825 14:17:20.677744   10304 system_pods.go:61] "kube-controller-manager-minikube" [69fc9a7d-f1ff-49dd-8437-bfd7cd844e37] Running
I0825 14:17:20.677744   10304 system_pods.go:61] "kube-proxy-7nlx5" [d5a4d672-7852-4c9c-8b8e-dff4623e5ebf] Pending / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0825 14:17:20.677744   10304 system_pods.go:61] "kube-scheduler-minikube" [73c3e797-acee-4bf0-828d-f7c53f079715] Running
I0825 14:17:20.677744   10304 system_pods.go:61] "storage-provisioner" [4c82e6d5-7a7e-4908-8fe1-16f59ee4b934] Pending
I0825 14:17:20.677744   10304 system_pods.go:74] duration metric: took 1.5169408s to wait for pod list to return data ...
I0825 14:17:20.677744   10304 kubeadm.go:578] duration metric: took 7.7003333s to wait for: map[apiserver:true system_pods:true]
I0825 14:17:20.677744   10304 node_conditions.go:102] verifying NodePressure condition ...
I0825 14:17:21.273206   10304 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0825 14:17:21.273206   10304 node_conditions.go:123] node cpu capacity is 8
I0825 14:17:21.376287   10304 node_conditions.go:105] duration metric: took 675.8473ms to run NodePressure ...
I0825 14:17:21.376287   10304 start.go:241] waiting for startup goroutines ...
I0825 14:17:21.376287   10304 start.go:246] waiting for cluster config update ...
I0825 14:17:21.376287   10304 start.go:255] writing updated cluster config ...
I0825 14:17:21.505791   10304 ssh_runner.go:195] Run: rm -f paused
I0825 14:17:41.539637   10304 start.go:607] kubectl: 1.32.2, cluster: 1.33.1 (minor skew: 1)
I0825 14:17:41.624889   10304 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Aug 25 08:45:57 minikube systemd[1]: docker.service: Consumed 1.082s CPU time.
Aug 25 08:45:57 minikube systemd[1]: Starting Docker Application Container Engine...
Aug 25 08:45:57 minikube dockerd[1332]: time="2025-08-25T08:45:57.634519899Z" level=info msg="Starting up"
Aug 25 08:45:57 minikube dockerd[1332]: time="2025-08-25T08:45:57.635600780Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Aug 25 08:45:57 minikube dockerd[1332]: time="2025-08-25T08:45:57.659509974Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Aug 25 08:45:57 minikube dockerd[1332]: time="2025-08-25T08:45:57.768272832Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Aug 25 08:45:57 minikube dockerd[1332]: time="2025-08-25T08:45:57.894085770Z" level=info msg="Loading containers: start."
Aug 25 08:45:59 minikube dockerd[1332]: time="2025-08-25T08:45:59.651121407Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 14b463c69107766124fc7a44502b2b0d7efa935616401e5ce9feca1e013d2373], retrying...."
Aug 25 08:46:00 minikube dockerd[1332]: time="2025-08-25T08:46:00.107833335Z" level=info msg="Loading containers: done."
Aug 25 08:46:00 minikube dockerd[1332]: time="2025-08-25T08:46:00.274795579Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
Aug 25 08:46:00 minikube dockerd[1332]: time="2025-08-25T08:46:00.275283817Z" level=info msg="Initializing buildkit"
Aug 25 08:46:00 minikube dockerd[1332]: time="2025-08-25T08:46:00.345091255Z" level=info msg="Completed buildkit initialization"
Aug 25 08:46:00 minikube dockerd[1332]: time="2025-08-25T08:46:00.352615551Z" level=info msg="Daemon has completed initialization"
Aug 25 08:46:00 minikube dockerd[1332]: time="2025-08-25T08:46:00.352744362Z" level=info msg="API listen on [::]:2376"
Aug 25 08:46:00 minikube dockerd[1332]: time="2025-08-25T08:46:00.352794966Z" level=info msg="API listen on /var/run/docker.sock"
Aug 25 08:46:00 minikube systemd[1]: Started Docker Application Container Engine.
Aug 25 08:46:00 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Aug 25 08:46:01 minikube cri-dockerd[1641]: time="2025-08-25T08:46:01Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Aug 25 08:46:01 minikube cri-dockerd[1641]: time="2025-08-25T08:46:01Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Aug 25 08:46:01 minikube cri-dockerd[1641]: time="2025-08-25T08:46:01Z" level=info msg="Start docker client with request timeout 0s"
Aug 25 08:46:01 minikube cri-dockerd[1641]: time="2025-08-25T08:46:01Z" level=info msg="Hairpin mode is set to hairpin-veth"
Aug 25 08:46:01 minikube cri-dockerd[1641]: time="2025-08-25T08:46:01Z" level=info msg="Loaded network plugin cni"
Aug 25 08:46:01 minikube cri-dockerd[1641]: time="2025-08-25T08:46:01Z" level=info msg="Docker cri networking managed by network plugin cni"
Aug 25 08:46:01 minikube cri-dockerd[1641]: time="2025-08-25T08:46:01Z" level=info msg="Setting cgroupDriver cgroupfs"
Aug 25 08:46:01 minikube cri-dockerd[1641]: time="2025-08-25T08:46:01Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Aug 25 08:46:01 minikube cri-dockerd[1641]: time="2025-08-25T08:46:01Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Aug 25 08:46:01 minikube cri-dockerd[1641]: time="2025-08-25T08:46:01Z" level=info msg="Start cri-dockerd grpc backend"
Aug 25 08:46:01 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Aug 25 08:46:25 minikube cri-dockerd[1641]: time="2025-08-25T08:46:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7a4f88e6a4f68aa633a7d9a4cf915d0fa486a75627b20522b0a1dd6e44f61b33/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 25 08:46:25 minikube cri-dockerd[1641]: time="2025-08-25T08:46:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2bd869641b26236a043cbd448dbb778b1839b4c63f08cd7b6c590e23285a2829/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 25 08:46:26 minikube cri-dockerd[1641]: time="2025-08-25T08:46:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5dd4b71c0d5029a18e89c31ca3427a0d327038dc716533dfd26644b97c88e279/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 25 08:46:26 minikube cri-dockerd[1641]: time="2025-08-25T08:46:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8d5f2fe61d6519908758a712f969d15b24cb53ddfa508b0bc730d29780e410a7/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 25 08:46:42 minikube dockerd[1332]: time="2025-08-25T08:46:42.585856844Z" level=info msg="ignoring event" container=7d4893468bc2f8545f3f86a6c88fcc5a552798bb5e923a065237447fe57462ed module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 25 08:47:12 minikube cri-dockerd[1641]: time="2025-08-25T08:47:12Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Aug 25 08:47:15 minikube cri-dockerd[1641]: time="2025-08-25T08:47:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2878d2405f54afe13ce5fd573fbde49b29517182da263f561dce4fda02a9ff0e/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 25 08:47:18 minikube cri-dockerd[1641]: time="2025-08-25T08:47:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/aa964450c681184ca3c96446b6c2f35af261536ae8a56eb51b505316ae4650ce/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 25 08:47:18 minikube cri-dockerd[1641]: time="2025-08-25T08:47:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/468c23ec601a952cb228c4c25a9c153f2e60f8dd5bc831558f1fc10cb11f497d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 25 08:47:28 minikube cri-dockerd[1641]: time="2025-08-25T08:47:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dc371c7731c1002cf01a7c2cf5c1fa1cce450e483b64942a405abfa7cf9ffb28/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 25 08:47:37 minikube dockerd[1332]: time="2025-08-25T08:47:37.995576254Z" level=info msg="ignoring event" container=008667ab6d11968c59d3cf0dd8e738c36de94ed51573776e538bd655b1e26af9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 25 08:47:39 minikube dockerd[1332]: time="2025-08-25T08:47:39.099680635Z" level=info msg="ignoring event" container=468c23ec601a952cb228c4c25a9c153f2e60f8dd5bc831558f1fc10cb11f497d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 25 08:47:57 minikube dockerd[1332]: time="2025-08-25T08:47:57.259634114Z" level=info msg="ignoring event" container=842d7f7c7cd565a0362989e6add8840bd0b7ad90b45eaa39b2d43b6606ae7efe module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 25 08:49:28 minikube cri-dockerd[1641]: time="2025-08-25T08:49:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3f21c13b01a8cc10907148204f777b6fdb0da37e889ea2d7914eb5b3ae2f6293/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 25 08:49:28 minikube cri-dockerd[1641]: time="2025-08-25T08:49:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ef17306c12b7a3322a88528bff34478db9e749d3a96d39d45a6548101b8de9aa/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 25 08:49:29 minikube dockerd[1332]: time="2025-08-25T08:49:29.934803211Z" level=warning msg="reference for unknown type: " digest="sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c" remote="docker.io/kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Aug 25 08:49:41 minikube cri-dockerd[1641]: time="2025-08-25T08:49:41Z" level=info msg="Stop pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: Status: Downloaded newer image for kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Aug 25 08:49:42 minikube dockerd[1332]: time="2025-08-25T08:49:42.328613835Z" level=warning msg="reference for unknown type: " digest="sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93" remote="docker.io/kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Aug 25 08:49:54 minikube cri-dockerd[1641]: time="2025-08-25T08:49:54Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Extracting [>                                                  ]  557.1kB/75.78MB"
Aug 25 08:50:04 minikube cri-dockerd[1641]: time="2025-08-25T08:50:04Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Extracting [===============================================>   ]   71.3MB/75.78MB"
Aug 25 08:50:08 minikube cri-dockerd[1641]: time="2025-08-25T08:50:08Z" level=info msg="Stop pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: Status: Downloaded newer image for kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Aug 25 09:00:40 minikube cri-dockerd[1641]: time="2025-08-25T09:00:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/96fc5dff869a9bb67d0ffdef369ca8e78a11a05a654f1a3ef851f1ed5e8b5b30/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 25 09:00:44 minikube dockerd[1332]: time="2025-08-25T09:00:44.539096834Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 25 09:00:44 minikube dockerd[1332]: time="2025-08-25T09:00:44.539665447Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 25 09:01:03 minikube dockerd[1332]: time="2025-08-25T09:01:03.322752226Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 25 09:01:03 minikube dockerd[1332]: time="2025-08-25T09:01:03.323080549Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 25 09:01:34 minikube dockerd[1332]: time="2025-08-25T09:01:34.679257596Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 25 09:01:34 minikube dockerd[1332]: time="2025-08-25T09:01:34.679372402Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 25 09:02:27 minikube dockerd[1332]: time="2025-08-25T09:02:27.330731346Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 25 09:02:27 minikube dockerd[1332]: time="2025-08-25T09:02:27.330808849Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 25 09:03:52 minikube dockerd[1332]: time="2025-08-25T09:03:52.762090359Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 25 09:03:52 minikube dockerd[1332]: time="2025-08-25T09:03:52.762287864Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE                                                                                                  CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
35a768e65284e       kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93         14 minutes ago      Running             kubernetes-dashboard        0                   ef17306c12b7a       kubernetes-dashboard-7779f9b69b-j2xvg
e74ac410e98e9       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c   14 minutes ago      Running             dashboard-metrics-scraper   0                   3f21c13b01a8c       dashboard-metrics-scraper-5d59dccf9b-t6pxn
ef65bc3ff63a3       6e38f40d628db                                                                                          16 minutes ago      Running             storage-provisioner         1                   dc371c7731c10       storage-provisioner
842d7f7c7cd56       6e38f40d628db                                                                                          16 minutes ago      Exited              storage-provisioner         0                   dc371c7731c10       storage-provisioner
d38a472128b53       1cf5f116067c6                                                                                          17 minutes ago      Running             coredns                     0                   aa964450c6811       coredns-674b8bbfcf-ksw6k
b4705458c70b0       b79c189b052cd                                                                                          17 minutes ago      Running             kube-proxy                  0                   2878d2405f54a       kube-proxy-7nlx5
a3b41e3489f1f       ef43894fa110c                                                                                          17 minutes ago      Running             kube-controller-manager     1                   2bd869641b262       kube-controller-manager-minikube
869318d0b1b47       398c985c0d950                                                                                          18 minutes ago      Running             kube-scheduler              0                   8d5f2fe61d651       kube-scheduler-minikube
6a04eb72ed968       499038711c081                                                                                          18 minutes ago      Running             etcd                        0                   5dd4b71c0d502       etcd-minikube
b72a040babd48       c6ab243b29f82                                                                                          18 minutes ago      Running             kube-apiserver              0                   7a4f88e6a4f68       kube-apiserver-minikube
7d4893468bc2f       ef43894fa110c                                                                                          18 minutes ago      Exited              kube-controller-manager     0                   2bd869641b262       kube-controller-manager-minikube


==> coredns [d38a472128b5] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[ERROR] plugin/errors: 2 3847973181225821364.3843277815584799216. HINFO: read udp 10.244.0.3:55577->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 3847973181225821364.3843277815584799216. HINFO: read udp 10.244.0.3:33685->192.168.65.254:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[ERROR] plugin/errors: 2 3847973181225821364.3843277815584799216. HINFO: read udp 10.244.0.3:48630->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 3847973181225821364.3843277815584799216. HINFO: read udp 10.244.0.3:56279->192.168.65.254:53: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] Reloading
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
[INFO] Reloading complete
[INFO] 127.0.0.1:42763 - 46640 "HINFO IN 4850787567612437669.811102311509957692. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.084947646s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_08_25T14_17_03_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 25 Aug 2025 08:46:32 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 25 Aug 2025 09:04:27 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 25 Aug 2025 09:00:49 +0000   Mon, 25 Aug 2025 08:46:32 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 25 Aug 2025 09:00:49 +0000   Mon, 25 Aug 2025 08:46:32 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 25 Aug 2025 09:00:49 +0000   Mon, 25 Aug 2025 08:46:32 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 25 Aug 2025 09:00:49 +0000   Mon, 25 Aug 2025 08:46:40 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3893476Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3893476Ki
  pods:               110
System Info:
  Machine ID:                 c47045f4e2da4d7e8f86b19af389bf2f
  System UUID:                c47045f4e2da4d7e8f86b19af389bf2f
  Boot ID:                    e94299e7-1e7d-4c78-a348-8174256ab71c
  Kernel Version:             6.6.87.2-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     hello-node-6ff47567bc-6gn2m                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m53s
  kube-system                 coredns-674b8bbfcf-ksw6k                      100m (1%)     0 (0%)      70Mi (1%)        170Mi (4%)     17m
  kube-system                 etcd-minikube                                 100m (1%)     0 (0%)      100Mi (2%)       0 (0%)         17m
  kube-system                 kube-apiserver-minikube                       250m (3%)     0 (0%)      0 (0%)           0 (0%)         17m
  kube-system                 kube-controller-manager-minikube              200m (2%)     0 (0%)      0 (0%)           0 (0%)         17m
  kube-system                 kube-proxy-7nlx5                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m
  kube-system                 kube-scheduler-minikube                       100m (1%)     0 (0%)      0 (0%)           0 (0%)         17m
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m
  kubernetes-dashboard        dashboard-metrics-scraper-5d59dccf9b-t6pxn    0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m
  kubernetes-dashboard        kubernetes-dashboard-7779f9b69b-j2xvg         0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%)   0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 16m                kube-proxy       
  Normal  Starting                 18m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  18m (x8 over 18m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    18m (x8 over 18m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     18m (x7 over 18m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  18m                kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 17m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  17m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  17m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    17m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     17m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           17m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Aug24 17:32] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.007682] PCI: Fatal: No config space access function found
[  +0.035190] PCI: System does not support PCI
[  +0.148872] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +2.012397] hv_balloon: Cold memory discard hypercall failed with status 1a00000005
[  +0.000836] hv_balloon: Underlying Hyper-V does not support order less than 9. Hypercall failed
[  +0.000871] hv_balloon: Defaulting to page_reporting_order 9
[  +8.637189] WSL (216) ERROR: CheckConnection: connect() failed: 101
[Aug24 17:33] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2119: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.121561] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +4.439934] pulseaudio[259]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +6.908222] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.019775] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002936] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002850] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.003530] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[ +12.836493] netlink: 'init': attribute type 4 has an invalid length.
[Aug24 19:34] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[Aug24 19:56] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[Aug24 23:30] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +5.252109] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[Aug24 23:35] WSL (216) ERROR: CheckConnection: connect() failed: 101
[Aug24 23:59] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[Aug25 01:27] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[Aug25 03:01] hrtimer: interrupt took 14427955 ns
[Aug25 03:05] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[Aug25 03:06] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[Aug25 03:07] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +4.684792] WSL (216) ERROR: CheckConnection: getaddrinfo() failed: -3
[  +0.785463] WSL (216) ERROR: CheckConnection: connect() failed: 101
[Aug25 04:37] WSL (216) ERROR: CheckConnection: connect() failed: 101
[Aug25 07:23] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[ +18.729429] WSL (216) ERROR: CheckConnection: connect() failed: 101


==> etcd [6a04eb72ed96] <==
{"level":"warn","ts":"2025-08-25T09:02:44.556820Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-25T09:02:44.247385Z","time spent":"309.418341ms","remote":"127.0.0.1:57694","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-08-25T09:02:44.853987Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"122.022792ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-25T09:02:44.854069Z","caller":"traceutil/trace.go:171","msg":"trace[1046808890] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1312; }","duration":"122.110396ms","start":"2025-08-25T09:02:44.731943Z","end":"2025-08-25T09:02:44.854054Z","steps":["trace[1046808890] 'range keys from in-memory index tree'  (duration: 121.950589ms)"],"step_count":1}
{"level":"info","ts":"2025-08-25T09:02:45.848244Z","caller":"traceutil/trace.go:171","msg":"trace[65008944] linearizableReadLoop","detail":"{readStateIndex:1522; appliedIndex:1521; }","duration":"115.771291ms","start":"2025-08-25T09:02:45.732455Z","end":"2025-08-25T09:02:45.848226Z","steps":["trace[65008944] 'read index received'  (duration: 115.571181ms)","trace[65008944] 'applied index is now lower than readState.Index'  (duration: 199.61µs)"],"step_count":2}
{"level":"warn","ts":"2025-08-25T09:02:45.848382Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"115.895097ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-25T09:02:45.848439Z","caller":"traceutil/trace.go:171","msg":"trace[36087438] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1313; }","duration":"115.9727ms","start":"2025-08-25T09:02:45.732450Z","end":"2025-08-25T09:02:45.848423Z","steps":["trace[36087438] 'agreement among raft nodes before linearized reading'  (duration: 115.874196ms)"],"step_count":1}
{"level":"info","ts":"2025-08-25T09:02:45.848490Z","caller":"traceutil/trace.go:171","msg":"trace[1761828925] transaction","detail":"{read_only:false; response_revision:1313; number_of_response:1; }","duration":"171.750193ms","start":"2025-08-25T09:02:45.676710Z","end":"2025-08-25T09:02:45.848461Z","steps":["trace[1761828925] 'process raft request'  (duration: 171.385476ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-25T09:02:46.364919Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"132.167782ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" limit:1 ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2025-08-25T09:02:46.365120Z","caller":"traceutil/trace.go:171","msg":"trace[1673615956] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:1313; }","duration":"132.376692ms","start":"2025-08-25T09:02:46.232703Z","end":"2025-08-25T09:02:46.365080Z","steps":["trace[1673615956] 'range keys from in-memory index tree'  (duration: 131.956672ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-25T09:02:46.830921Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"228.319426ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128039517087111049 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1304 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128039517087111046 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2025-08-25T09:02:46.831521Z","caller":"traceutil/trace.go:171","msg":"trace[1598999394] transaction","detail":"{read_only:false; response_revision:1314; number_of_response:1; }","duration":"338.285735ms","start":"2025-08-25T09:02:46.493167Z","end":"2025-08-25T09:02:46.831452Z","steps":["trace[1598999394] 'process raft request'  (duration: 109.156771ms)","trace[1598999394] 'compare'  (duration: 227.986709ms)"],"step_count":2}
{"level":"warn","ts":"2025-08-25T09:02:46.831935Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-25T09:02:46.493099Z","time spent":"338.590851ms","remote":"127.0.0.1:57728","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1304 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128039517087111046 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2025-08-25T09:02:48.128767Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"117.697884ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/volumeattachments/\" range_end:\"/registry/volumeattachments0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-25T09:02:48.128991Z","caller":"traceutil/trace.go:171","msg":"trace[984483563] range","detail":"{range_begin:/registry/volumeattachments/; range_end:/registry/volumeattachments0; response_count:0; response_revision:1315; }","duration":"117.836191ms","start":"2025-08-25T09:02:48.011022Z","end":"2025-08-25T09:02:48.128858Z","steps":["trace[984483563] 'count revisions from in-memory index tree'  (duration: 117.63148ms)"],"step_count":1}
{"level":"info","ts":"2025-08-25T09:02:50.144059Z","caller":"traceutil/trace.go:171","msg":"trace[1571858476] transaction","detail":"{read_only:false; response_revision:1317; number_of_response:1; }","duration":"207.808135ms","start":"2025-08-25T09:02:49.936236Z","end":"2025-08-25T09:02:50.144044Z","steps":["trace[1571858476] 'process raft request'  (duration: 207.71143ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-25T09:02:54.586036Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"113.615093ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128039517087111085 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:1309 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:16"}
{"level":"info","ts":"2025-08-25T09:02:54.586210Z","caller":"traceutil/trace.go:171","msg":"trace[2004081746] linearizableReadLoop","detail":"{readStateIndex:1531; appliedIndex:1530; }","duration":"151.063105ms","start":"2025-08-25T09:02:54.435124Z","end":"2025-08-25T09:02:54.586187Z","steps":["trace[2004081746] 'read index received'  (duration: 37.42371ms)","trace[2004081746] 'applied index is now lower than readState.Index'  (duration: 113.637494ms)"],"step_count":2}
{"level":"info","ts":"2025-08-25T09:02:54.586224Z","caller":"traceutil/trace.go:171","msg":"trace[1072637730] transaction","detail":"{read_only:false; response_revision:1320; number_of_response:1; }","duration":"215.764833ms","start":"2025-08-25T09:02:54.370429Z","end":"2025-08-25T09:02:54.586194Z","steps":["trace[1072637730] 'process raft request'  (duration: 101.900527ms)","trace[1072637730] 'compare'  (duration: 113.278378ms)"],"step_count":2}
{"level":"warn","ts":"2025-08-25T09:02:54.586309Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"151.16901ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/\" range_end:\"/registry/namespaces0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-08-25T09:02:54.586351Z","caller":"traceutil/trace.go:171","msg":"trace[1316224749] range","detail":"{range_begin:/registry/namespaces/; range_end:/registry/namespaces0; response_count:0; response_revision:1320; }","duration":"151.233613ms","start":"2025-08-25T09:02:54.435104Z","end":"2025-08-25T09:02:54.586338Z","steps":["trace[1316224749] 'agreement among raft nodes before linearized reading'  (duration: 151.151409ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-25T09:02:54.879172Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"148.670889ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-25T09:02:54.879285Z","caller":"traceutil/trace.go:171","msg":"trace[166836014] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1320; }","duration":"148.783594ms","start":"2025-08-25T09:02:54.730469Z","end":"2025-08-25T09:02:54.879253Z","steps":["trace[166836014] 'range keys from in-memory index tree'  (duration: 148.594385ms)"],"step_count":1}
{"level":"info","ts":"2025-08-25T09:02:55.774186Z","caller":"traceutil/trace.go:171","msg":"trace[2038093864] transaction","detail":"{read_only:false; response_revision:1321; number_of_response:1; }","duration":"152.61608ms","start":"2025-08-25T09:02:55.621546Z","end":"2025-08-25T09:02:55.774162Z","steps":["trace[2038093864] 'process raft request'  (duration: 78.518797ms)","trace[2038093864] 'compare'  (duration: 73.977377ms)"],"step_count":2}
{"level":"info","ts":"2025-08-25T09:02:55.798724Z","caller":"traceutil/trace.go:171","msg":"trace[2019946426] transaction","detail":"{read_only:false; response_revision:1322; number_of_response:1; }","duration":"145.291825ms","start":"2025-08-25T09:02:55.653411Z","end":"2025-08-25T09:02:55.798703Z","steps":["trace[2019946426] 'process raft request'  (duration: 145.137218ms)"],"step_count":1}
{"level":"info","ts":"2025-08-25T09:02:58.628907Z","caller":"traceutil/trace.go:171","msg":"trace[151623823] linearizableReadLoop","detail":"{readStateIndex:1539; appliedIndex:1538; }","duration":"209.332222ms","start":"2025-08-25T09:02:58.419522Z","end":"2025-08-25T09:02:58.628854Z","steps":["trace[151623823] 'read index received'  (duration: 208.838698ms)","trace[151623823] 'applied index is now lower than readState.Index'  (duration: 491.024µs)"],"step_count":2}
{"level":"warn","ts":"2025-08-25T09:02:58.629659Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"210.090458ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-25T09:02:58.629610Z","caller":"traceutil/trace.go:171","msg":"trace[1011901329] transaction","detail":"{read_only:false; response_revision:1326; number_of_response:1; }","duration":"289.671806ms","start":"2025-08-25T09:02:58.339769Z","end":"2025-08-25T09:02:58.629441Z","steps":["trace[1011901329] 'process raft request'  (duration: 288.192435ms)"],"step_count":1}
{"level":"info","ts":"2025-08-25T09:02:58.629813Z","caller":"traceutil/trace.go:171","msg":"trace[1887387510] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1326; }","duration":"210.31747ms","start":"2025-08-25T09:02:58.419460Z","end":"2025-08-25T09:02:58.629777Z","steps":["trace[1887387510] 'agreement among raft nodes before linearized reading'  (duration: 209.800045ms)"],"step_count":1}
{"level":"info","ts":"2025-08-25T09:02:58.699565Z","caller":"traceutil/trace.go:171","msg":"trace[404034207] transaction","detail":"{read_only:false; response_revision:1327; number_of_response:1; }","duration":"232.912862ms","start":"2025-08-25T09:02:58.466596Z","end":"2025-08-25T09:02:58.699509Z","steps":["trace[404034207] 'process raft request'  (duration: 232.339835ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-25T09:02:58.902244Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"171.785006ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-25T09:02:58.902496Z","caller":"traceutil/trace.go:171","msg":"trace[314424758] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1327; }","duration":"172.057719ms","start":"2025-08-25T09:02:58.730401Z","end":"2025-08-25T09:02:58.902459Z","steps":["trace[314424758] 'range keys from in-memory index tree'  (duration: 171.733303ms)"],"step_count":1}
{"level":"info","ts":"2025-08-25T09:03:11.171256Z","caller":"traceutil/trace.go:171","msg":"trace[576592710] transaction","detail":"{read_only:false; response_revision:1338; number_of_response:1; }","duration":"188.95084ms","start":"2025-08-25T09:03:10.982241Z","end":"2025-08-25T09:03:11.171192Z","steps":["trace[576592710] 'process raft request'  (duration: 188.722729ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-25T09:03:11.607416Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"262.891696ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128039517087111180 > lease_revoke:<id:70cc98e0685097be>","response":"size:29"}
{"level":"info","ts":"2025-08-25T09:03:11.607743Z","caller":"traceutil/trace.go:171","msg":"trace[1329640044] linearizableReadLoop","detail":"{readStateIndex:1554; appliedIndex:1553; }","duration":"200.200396ms","start":"2025-08-25T09:03:11.407504Z","end":"2025-08-25T09:03:11.607705Z","steps":["trace[1329640044] 'read index received'  (duration: 20.201µs)","trace[1329640044] 'applied index is now lower than readState.Index'  (duration: 200.176495ms)"],"step_count":2}
{"level":"warn","ts":"2025-08-25T09:03:11.607937Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"200.399607ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-25T09:03:11.608096Z","caller":"traceutil/trace.go:171","msg":"trace[475469278] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1338; }","duration":"200.580715ms","start":"2025-08-25T09:03:11.407486Z","end":"2025-08-25T09:03:11.608067Z","steps":["trace[475469278] 'agreement among raft nodes before linearized reading'  (duration: 200.342003ms)"],"step_count":1}
{"level":"info","ts":"2025-08-25T09:03:13.326542Z","caller":"traceutil/trace.go:171","msg":"trace[301963047] transaction","detail":"{read_only:false; response_revision:1339; number_of_response:1; }","duration":"146.46074ms","start":"2025-08-25T09:03:13.180060Z","end":"2025-08-25T09:03:13.326521Z","steps":["trace[301963047] 'process raft request'  (duration: 146.240229ms)"],"step_count":1}
{"level":"info","ts":"2025-08-25T09:03:15.480433Z","caller":"traceutil/trace.go:171","msg":"trace[943531332] transaction","detail":"{read_only:false; response_revision:1341; number_of_response:1; }","duration":"137.518098ms","start":"2025-08-25T09:03:15.342899Z","end":"2025-08-25T09:03:15.480417Z","steps":["trace[943531332] 'process raft request'  (duration: 57.534544ms)","trace[943531332] 'compare'  (duration: 79.919551ms)"],"step_count":2}
{"level":"info","ts":"2025-08-25T09:03:17.704825Z","caller":"traceutil/trace.go:171","msg":"trace[774854537] transaction","detail":"{read_only:false; response_revision:1343; number_of_response:1; }","duration":"216.311845ms","start":"2025-08-25T09:03:17.489901Z","end":"2025-08-25T09:03:17.704807Z","steps":["trace[774854537] 'process raft request'  (duration: 216.150038ms)"],"step_count":1}
{"level":"info","ts":"2025-08-25T09:03:19.416769Z","caller":"traceutil/trace.go:171","msg":"trace[884254645] transaction","detail":"{read_only:false; response_revision:1344; number_of_response:1; }","duration":"120.11879ms","start":"2025-08-25T09:03:19.296603Z","end":"2025-08-25T09:03:19.416722Z","steps":["trace[884254645] 'process raft request'  (duration: 119.733672ms)"],"step_count":1}
{"level":"info","ts":"2025-08-25T09:03:19.848034Z","caller":"traceutil/trace.go:171","msg":"trace[2109971520] linearizableReadLoop","detail":"{readStateIndex:1562; appliedIndex:1561; }","duration":"118.935334ms","start":"2025-08-25T09:03:19.729085Z","end":"2025-08-25T09:03:19.848020Z","steps":["trace[2109971520] 'read index received'  (duration: 118.822829ms)","trace[2109971520] 'applied index is now lower than readState.Index'  (duration: 112.105µs)"],"step_count":2}
{"level":"info","ts":"2025-08-25T09:03:19.848127Z","caller":"traceutil/trace.go:171","msg":"trace[214389842] transaction","detail":"{read_only:false; response_revision:1345; number_of_response:1; }","duration":"129.175219ms","start":"2025-08-25T09:03:19.718944Z","end":"2025-08-25T09:03:19.848120Z","steps":["trace[214389842] 'process raft request'  (duration: 129.004011ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-25T09:03:19.848303Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"119.196047ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-25T09:03:19.848364Z","caller":"traceutil/trace.go:171","msg":"trace[2036579151] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1345; }","duration":"119.27245ms","start":"2025-08-25T09:03:19.729080Z","end":"2025-08-25T09:03:19.848353Z","steps":["trace[2036579151] 'agreement among raft nodes before linearized reading'  (duration: 119.178346ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-25T09:03:21.504735Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"127.278329ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128039517087111229 > lease_revoke:<id:70cc98e0685097f0>","response":"size:29"}
{"level":"info","ts":"2025-08-25T09:03:21.504923Z","caller":"traceutil/trace.go:171","msg":"trace[161539138] linearizableReadLoop","detail":"{readStateIndex:1563; appliedIndex:1562; }","duration":"187.194267ms","start":"2025-08-25T09:03:21.317695Z","end":"2025-08-25T09:03:21.504889Z","steps":["trace[161539138] 'read index received'  (duration: 59.675927ms)","trace[161539138] 'applied index is now lower than readState.Index'  (duration: 127.51664ms)"],"step_count":2}
{"level":"warn","ts":"2025-08-25T09:03:21.505216Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"187.46698ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/jobs/\" range_end:\"/registry/jobs0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-25T09:03:21.505381Z","caller":"traceutil/trace.go:171","msg":"trace[428402947] range","detail":"{range_begin:/registry/jobs/; range_end:/registry/jobs0; response_count:0; response_revision:1345; }","duration":"187.659689ms","start":"2025-08-25T09:03:21.317671Z","end":"2025-08-25T09:03:21.505330Z","steps":["trace[428402947] 'agreement among raft nodes before linearized reading'  (duration: 187.403177ms)"],"step_count":1}
{"level":"info","ts":"2025-08-25T09:03:24.133634Z","caller":"traceutil/trace.go:171","msg":"trace[2127747476] transaction","detail":"{read_only:false; response_revision:1349; number_of_response:1; }","duration":"184.756752ms","start":"2025-08-25T09:03:23.948840Z","end":"2025-08-25T09:03:24.133597Z","steps":["trace[2127747476] 'process raft request'  (duration: 184.521441ms)"],"step_count":1}
{"level":"info","ts":"2025-08-25T09:03:44.823334Z","caller":"traceutil/trace.go:171","msg":"trace[739632012] transaction","detail":"{read_only:false; response_revision:1367; number_of_response:1; }","duration":"137.434279ms","start":"2025-08-25T09:03:44.685885Z","end":"2025-08-25T09:03:44.823320Z","steps":["trace[739632012] 'process raft request'  (duration: 137.340574ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-25T09:03:46.484894Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.51568ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128039517087111362 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1361 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128039517087111360 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2025-08-25T09:03:46.484988Z","caller":"traceutil/trace.go:171","msg":"trace[1974160775] transaction","detail":"{read_only:false; response_revision:1369; number_of_response:1; }","duration":"161.661556ms","start":"2025-08-25T09:03:46.323307Z","end":"2025-08-25T09:03:46.484968Z","steps":["trace[1974160775] 'process raft request'  (duration: 57.015071ms)","trace[1974160775] 'compare'  (duration: 104.456176ms)"],"step_count":2}
{"level":"info","ts":"2025-08-25T09:03:59.256808Z","caller":"traceutil/trace.go:171","msg":"trace[1918529914] transaction","detail":"{read_only:false; response_revision:1382; number_of_response:1; }","duration":"109.238468ms","start":"2025-08-25T09:03:59.147542Z","end":"2025-08-25T09:03:59.256781Z","steps":["trace[1918529914] 'process raft request'  (duration: 109.05525ms)"],"step_count":1}
{"level":"info","ts":"2025-08-25T09:04:07.801510Z","caller":"traceutil/trace.go:171","msg":"trace[1982670292] transaction","detail":"{read_only:false; response_revision:1391; number_of_response:1; }","duration":"121.831536ms","start":"2025-08-25T09:04:07.679661Z","end":"2025-08-25T09:04:07.801492Z","steps":["trace[1982670292] 'process raft request'  (duration: 72.620455ms)","trace[1982670292] 'compare'  (duration: 49.058472ms)"],"step_count":2}
{"level":"warn","ts":"2025-08-25T09:04:16.522248Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"130.774098ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128039517087111522 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1387 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128039517087111520 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2025-08-25T09:04:16.522622Z","caller":"traceutil/trace.go:171","msg":"trace[914371712] transaction","detail":"{read_only:false; response_revision:1398; number_of_response:1; }","duration":"154.435837ms","start":"2025-08-25T09:04:16.368140Z","end":"2025-08-25T09:04:16.522576Z","steps":["trace[914371712] 'process raft request'  (duration: 23.151714ms)","trace[914371712] 'compare'  (duration: 130.284974ms)"],"step_count":2}
{"level":"info","ts":"2025-08-25T09:04:20.736520Z","caller":"traceutil/trace.go:171","msg":"trace[316236040] transaction","detail":"{read_only:false; response_revision:1402; number_of_response:1; }","duration":"104.679298ms","start":"2025-08-25T09:04:20.631797Z","end":"2025-08-25T09:04:20.736477Z","steps":["trace[316236040] 'process raft request'  (duration: 104.365083ms)"],"step_count":1}
{"level":"info","ts":"2025-08-25T09:04:20.762400Z","caller":"traceutil/trace.go:171","msg":"trace[1302262731] transaction","detail":"{read_only:false; response_revision:1403; number_of_response:1; }","duration":"127.244597ms","start":"2025-08-25T09:04:20.635102Z","end":"2025-08-25T09:04:20.762347Z","steps":["trace[1302262731] 'process raft request'  (duration: 126.219148ms)"],"step_count":1}
{"level":"info","ts":"2025-08-25T09:04:24.200870Z","caller":"traceutil/trace.go:171","msg":"trace[1224455734] transaction","detail":"{read_only:false; response_revision:1407; number_of_response:1; }","duration":"217.810409ms","start":"2025-08-25T09:04:23.983044Z","end":"2025-08-25T09:04:24.200854Z","steps":["trace[1224455734] 'process raft request'  (duration: 217.6346ms)"],"step_count":1}
{"level":"info","ts":"2025-08-25T09:04:26.438203Z","caller":"traceutil/trace.go:171","msg":"trace[626218312] transaction","detail":"{read_only:false; response_revision:1409; number_of_response:1; }","duration":"136.74026ms","start":"2025-08-25T09:04:26.301372Z","end":"2025-08-25T09:04:26.438113Z","steps":["trace[626218312] 'process raft request'  (duration: 42.530872ms)","trace[626218312] 'compare'  (duration: 93.812269ms)"],"step_count":2}


==> kernel <==
 09:04:30 up 15:31,  0 users,  load average: 0.44, 0.72, 0.96
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [b72a040babd4] <==
I0825 08:46:32.546351       1 autoregister_controller.go:144] Starting autoregister controller
I0825 08:46:32.546386       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0825 08:46:32.546413       1 cache.go:39] Caches are synced for autoregister controller
I0825 08:46:32.547813       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0825 08:46:32.549260       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0825 08:46:32.551691       1 cache.go:39] Caches are synced for LocalAvailability controller
I0825 08:46:32.554277       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0825 08:46:32.554545       1 default_servicecidr_controller.go:165] Creating default ServiceCIDR with CIDRs: [10.96.0.0/12]
I0825 08:46:32.555436       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0825 08:46:32.640160       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0825 08:46:32.643542       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0825 08:46:32.643592       1 policy_source.go:240] refreshing policies
I0825 08:46:32.662504       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0825 08:46:32.662599       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0825 08:46:32.662619       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0825 08:46:32.662641       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0825 08:46:32.662660       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0825 08:46:32.664135       1 controller.go:667] quota admission added evaluator for: namespaces
I0825 08:46:32.924634       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0825 08:46:32.925471       1 default_servicecidr_controller.go:214] Setting default ServiceCIDR condition Ready to True
I0825 08:46:33.334259       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0825 08:46:33.380470       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0825 08:46:33.381094       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0825 08:46:33.383530       1 default_servicecidr_controller.go:214] Setting default ServiceCIDR condition Ready to True
I0825 08:46:33.864262       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0825 08:46:34.265441       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0825 08:46:34.935559       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0825 08:46:34.935793       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0825 08:46:53.401418       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0825 08:46:54.063183       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0825 08:46:55.160256       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
I0825 08:46:55.162763       1 controller.go:667] quota admission added evaluator for: serviceaccounts
W0825 08:46:55.588271       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0825 08:46:55.593619       1 controller.go:667] quota admission added evaluator for: endpoints
I0825 08:46:55.851534       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
E0825 08:46:59.102723       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"context canceled\"}: context canceled" logger="UnhandledError"
E0825 08:46:59.102778       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0825 08:46:59.102814       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 23.202µs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E0825 08:46:59.104043       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0825 08:46:59.104147       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0825 08:46:59.105276       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0825 08:46:59.105319       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0825 08:46:59.106523       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0825 08:46:59.106698       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="4.045645ms" method="PATCH" path="/api/v1/namespaces/kube-system/events/kube-scheduler-minikube.185ef632d45b7876" result=null
E0825 08:46:59.107793       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="5.083834ms" method="GET" path="/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube" result=null
I0825 08:47:01.222772       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0825 08:47:02.285255       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0825 08:47:02.566916       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I0825 08:47:07.048929       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0825 08:47:07.369878       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0825 08:47:07.388379       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0825 08:47:07.391414       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I0825 08:49:24.802117       1 alloc.go:328] "allocated clusterIPs" service="kubernetes-dashboard/kubernetes-dashboard" clusterIPs={"IPv4":"10.106.13.63"}
I0825 08:49:24.993634       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0825 08:49:25.801431       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0825 08:49:25.801411       1 alloc.go:328] "allocated clusterIPs" service="kubernetes-dashboard/dashboard-metrics-scraper" clusterIPs={"IPv4":"10.99.61.46"}
I0825 08:56:32.521316       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0825 09:01:20.080126       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0825 09:01:20.324819       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0825 09:01:20.324891       1 alloc.go:328] "allocated clusterIPs" service="default/hello-node" clusterIPs={"IPv4":"10.96.2.20"}


==> kube-controller-manager [7d4893468bc2] <==
I0825 08:46:29.058334       1 serving.go:386] Generated self-signed cert in-memory
I0825 08:46:29.688246       1 controllermanager.go:188] "Starting" version="v1.33.1"
I0825 08:46:29.688279       1 controllermanager.go:190] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0825 08:46:29.689632       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0825 08:46:29.689677       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0825 08:46:29.689960       1 secure_serving.go:211] Serving securely on 127.0.0.1:10257
I0825 08:46:29.690113       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0825 08:46:42.547037       1 controllermanager.go:242] "Error building controller context" err="failed to wait for apiserver being healthy: timed out waiting for the condition: failed to get apiserver /healthz status: forbidden: User \"system:kube-controller-manager\" cannot get path \"/healthz\""


==> kube-controller-manager [a3b41e3489f1] <==
I0825 08:47:06.529823       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0825 08:47:06.530649       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0825 08:47:06.554388       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0825 08:47:06.556736       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0825 08:47:06.560264       1 shared_informer.go:357] "Caches are synced" controller="node"
I0825 08:47:06.560330       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0825 08:47:06.560363       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0825 08:47:06.560398       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0825 08:47:06.560406       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0825 08:47:06.565887       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0825 08:47:06.565907       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0825 08:47:06.565969       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0825 08:47:06.567177       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0825 08:47:06.576922       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0825 08:47:06.578152       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0825 08:47:06.587702       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0825 08:47:06.591244       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0825 08:47:06.611213       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0825 08:47:06.611262       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0825 08:47:06.638404       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0825 08:47:06.706878       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0825 08:47:06.804174       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0825 08:47:06.807390       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0825 08:47:06.809684       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0825 08:47:06.813137       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0825 08:47:06.821148       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0825 08:47:06.825670       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0825 08:47:06.830037       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0825 08:47:06.842645       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0825 08:47:06.854057       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0825 08:47:06.854242       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0825 08:47:06.854380       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0825 08:47:06.854529       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0825 08:47:06.854726       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0825 08:47:06.859210       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0825 08:47:06.861789       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0825 08:47:06.865218       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0825 08:47:06.869666       1 shared_informer.go:357] "Caches are synced" controller="job"
I0825 08:47:06.891048       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0825 08:47:06.892638       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0825 08:47:06.892718       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0825 08:47:06.895008       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0825 08:47:06.896481       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0825 08:47:06.943211       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0825 08:47:07.320064       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0825 08:47:07.343155       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0825 08:47:07.343237       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0825 08:47:07.343254       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
E0825 08:49:21.256212       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0825 08:49:21.445119       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0825 08:49:21.512625       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-7779f9b69b\" failed with pods \"kubernetes-dashboard-7779f9b69b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0825 08:49:21.512658       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0825 08:49:21.654875       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-7779f9b69b\" failed with pods \"kubernetes-dashboard-7779f9b69b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0825 08:49:21.655012       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0825 08:49:21.740329       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-7779f9b69b\" failed with pods \"kubernetes-dashboard-7779f9b69b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0825 08:49:21.740594       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0825 08:49:21.879085       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-7779f9b69b\" failed with pods \"kubernetes-dashboard-7779f9b69b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0825 08:49:21.879116       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0825 08:49:21.931723       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0825 08:49:21.931752       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-7779f9b69b\" failed with pods \"kubernetes-dashboard-7779f9b69b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"


==> kube-proxy [b4705458c70b] <==
I0825 08:47:36.848183       1 server_linux.go:63] "Using iptables proxy"
I0825 08:47:38.610179       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0825 08:47:38.625739       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0825 08:47:39.524707       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0825 08:47:39.524851       1 server_linux.go:145] "Using iptables Proxier"
I0825 08:47:39.539443       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0825 08:47:39.753621       1 server.go:516] "Version info" version="v1.33.1"
I0825 08:47:39.753728       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0825 08:47:39.864267       1 config.go:199] "Starting service config controller"
I0825 08:47:39.864575       1 config.go:105] "Starting endpoint slice config controller"
I0825 08:47:39.906729       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0825 08:47:39.906778       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0825 08:47:39.938819       1 config.go:440] "Starting serviceCIDR config controller"
I0825 08:47:39.938914       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0825 08:47:39.939010       1 config.go:329] "Starting node config controller"
I0825 08:47:39.939046       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0825 08:47:40.039668       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0825 08:47:40.039779       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0825 08:47:40.107194       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0825 08:47:40.107339       1 shared_informer.go:357] "Caches are synced" controller="service config"


==> kube-scheduler [869318d0b1b4] <==
E0825 08:46:32.662187       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0825 08:46:32.662244       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0825 08:46:32.662230       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0825 08:46:32.662329       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0825 08:46:32.662445       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0825 08:46:32.662349       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0825 08:46:32.662441       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0825 08:46:32.662528       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0825 08:46:32.662553       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0825 08:46:33.518409       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0825 08:46:33.545098       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0825 08:46:33.553788       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0825 08:46:33.691734       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0825 08:46:33.706700       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0825 08:46:33.708306       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0825 08:46:33.716496       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0825 08:46:33.721363       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0825 08:46:33.736740       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0825 08:46:33.758105       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0825 08:46:33.800736       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0825 08:46:33.884378       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0825 08:46:33.973767       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0825 08:46:33.992603       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0825 08:46:34.231662       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0825 08:46:34.266203       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0825 08:46:35.218953       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0825 08:46:35.347505       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0825 08:46:35.412477       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0825 08:46:35.415606       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0825 08:46:35.443543       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0825 08:46:35.674641       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0825 08:46:35.685648       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0825 08:46:35.872128       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0825 08:46:35.888583       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0825 08:46:35.903843       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0825 08:46:36.284761       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0825 08:46:36.319426       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0825 08:46:36.484413       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0825 08:46:36.780235       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0825 08:46:36.944142       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0825 08:46:37.015161       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0825 08:46:39.014312       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0825 08:46:39.046289       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0825 08:46:39.286855       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0825 08:46:39.736850       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0825 08:46:40.083579       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0825 08:46:40.096333       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0825 08:46:40.259924       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0825 08:46:40.495849       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0825 08:46:40.738712       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0825 08:46:40.817389       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0825 08:46:40.866265       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0825 08:46:41.192650       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0825 08:46:41.413438       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0825 08:46:41.493196       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0825 08:46:42.519299       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0825 08:46:42.800868       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0825 08:46:45.902715       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0825 08:46:51.019372       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
I0825 08:47:12.157656       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Aug 25 08:47:12 minikube kubelet[2656]: I0825 08:47:12.759135    2656 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Aug 25 08:47:18 minikube kubelet[2656]: I0825 08:47:18.068742    2656 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="468c23ec601a952cb228c4c25a9c153f2e60f8dd5bc831558f1fc10cb11f497d"
Aug 25 08:47:18 minikube kubelet[2656]: I0825 08:47:18.077353    2656 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2878d2405f54afe13ce5fd573fbde49b29517182da263f561dce4fda02a9ff0e"
Aug 25 08:47:20 minikube kubelet[2656]: I0825 08:47:20.027403    2656 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/4c82e6d5-7a7e-4908-8fe1-16f59ee4b934-tmp\") pod \"storage-provisioner\" (UID: \"4c82e6d5-7a7e-4908-8fe1-16f59ee4b934\") " pod="kube-system/storage-provisioner"
Aug 25 08:47:20 minikube kubelet[2656]: I0825 08:47:20.027606    2656 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-8mhrs\" (UniqueName: \"kubernetes.io/projected/4c82e6d5-7a7e-4908-8fe1-16f59ee4b934-kube-api-access-8mhrs\") pod \"storage-provisioner\" (UID: \"4c82e6d5-7a7e-4908-8fe1-16f59ee4b934\") " pod="kube-system/storage-provisioner"
Aug 25 08:47:24 minikube kubelet[2656]: I0825 08:47:24.471162    2656 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-7nlx5" podStartSLOduration=17.471144296 podStartE2EDuration="17.471144296s" podCreationTimestamp="2025-08-25 08:47:07 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-08-25 08:47:24.471101093 +0000 UTC m=+24.080840268" watchObservedRunningTime="2025-08-25 08:47:24.471144296 +0000 UTC m=+24.080883471"
Aug 25 08:47:25 minikube kubelet[2656]: I0825 08:47:25.945874    2656 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-674b8bbfcf-x6hpz" podStartSLOduration=17.94585361 podStartE2EDuration="17.94585361s" podCreationTimestamp="2025-08-25 08:47:08 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-08-25 08:47:25.945518584 +0000 UTC m=+25.555257859" watchObservedRunningTime="2025-08-25 08:47:25.94585361 +0000 UTC m=+25.555592885"
Aug 25 08:47:28 minikube kubelet[2656]: I0825 08:47:28.014063    2656 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="dc371c7731c1002cf01a7c2cf5c1fa1cce450e483b64942a405abfa7cf9ffb28"
Aug 25 08:47:32 minikube kubelet[2656]: I0825 08:47:32.857725    2656 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-674b8bbfcf-ksw6k" podStartSLOduration=24.85768227 podStartE2EDuration="24.85768227s" podCreationTimestamp="2025-08-25 08:47:08 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-08-25 08:47:27.230129146 +0000 UTC m=+26.839868521" watchObservedRunningTime="2025-08-25 08:47:32.85768227 +0000 UTC m=+32.467421545"
Aug 25 08:47:40 minikube kubelet[2656]: I0825 08:47:40.321436    2656 scope.go:117] "RemoveContainer" containerID="008667ab6d11968c59d3cf0dd8e738c36de94ed51573776e538bd655b1e26af9"
Aug 25 08:47:40 minikube kubelet[2656]: I0825 08:47:40.439277    2656 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/a698c6ae-9dda-42a7-adeb-6de80a76913b-config-volume\") pod \"a698c6ae-9dda-42a7-adeb-6de80a76913b\" (UID: \"a698c6ae-9dda-42a7-adeb-6de80a76913b\") "
Aug 25 08:47:40 minikube kubelet[2656]: I0825 08:47:40.439422    2656 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-vv554\" (UniqueName: \"kubernetes.io/projected/a698c6ae-9dda-42a7-adeb-6de80a76913b-kube-api-access-vv554\") pod \"a698c6ae-9dda-42a7-adeb-6de80a76913b\" (UID: \"a698c6ae-9dda-42a7-adeb-6de80a76913b\") "
Aug 25 08:47:40 minikube kubelet[2656]: I0825 08:47:40.523475    2656 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/a698c6ae-9dda-42a7-adeb-6de80a76913b-config-volume" (OuterVolumeSpecName: "config-volume") pod "a698c6ae-9dda-42a7-adeb-6de80a76913b" (UID: "a698c6ae-9dda-42a7-adeb-6de80a76913b"). InnerVolumeSpecName "config-volume". PluginName "kubernetes.io/configmap", VolumeGIDValue ""
Aug 25 08:47:40 minikube kubelet[2656]: I0825 08:47:40.828851    2656 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/a698c6ae-9dda-42a7-adeb-6de80a76913b-kube-api-access-vv554" (OuterVolumeSpecName: "kube-api-access-vv554") pod "a698c6ae-9dda-42a7-adeb-6de80a76913b" (UID: "a698c6ae-9dda-42a7-adeb-6de80a76913b"). InnerVolumeSpecName "kube-api-access-vv554". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Aug 25 08:47:40 minikube kubelet[2656]: I0825 08:47:40.843237    2656 reconciler_common.go:299] "Volume detached for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/a698c6ae-9dda-42a7-adeb-6de80a76913b-config-volume\") on node \"minikube\" DevicePath \"\""
Aug 25 08:47:40 minikube kubelet[2656]: I0825 08:47:40.843280    2656 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-vv554\" (UniqueName: \"kubernetes.io/projected/a698c6ae-9dda-42a7-adeb-6de80a76913b-kube-api-access-vv554\") on node \"minikube\" DevicePath \"\""
Aug 25 08:47:40 minikube kubelet[2656]: I0825 08:47:40.866099    2656 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=22.86606185 podStartE2EDuration="22.86606185s" podCreationTimestamp="2025-08-25 08:47:18 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-08-25 08:47:32.858197511 +0000 UTC m=+32.467936786" watchObservedRunningTime="2025-08-25 08:47:40.86606185 +0000 UTC m=+40.475801025"
Aug 25 08:47:43 minikube kubelet[2656]: I0825 08:47:43.688232    2656 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="a698c6ae-9dda-42a7-adeb-6de80a76913b" path="/var/lib/kubelet/pods/a698c6ae-9dda-42a7-adeb-6de80a76913b/volumes"
Aug 25 08:47:58 minikube kubelet[2656]: I0825 08:47:58.530744    2656 scope.go:117] "RemoveContainer" containerID="842d7f7c7cd565a0362989e6add8840bd0b7ad90b45eaa39b2d43b6606ae7efe"
Aug 25 08:49:22 minikube kubelet[2656]: I0825 08:49:22.525995    2656 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-x4dth\" (UniqueName: \"kubernetes.io/projected/8c3f8cd9-a3a9-4355-acc5-231869b60936-kube-api-access-x4dth\") pod \"dashboard-metrics-scraper-5d59dccf9b-t6pxn\" (UID: \"8c3f8cd9-a3a9-4355-acc5-231869b60936\") " pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-t6pxn"
Aug 25 08:49:22 minikube kubelet[2656]: I0825 08:49:22.526211    2656 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-volume\" (UniqueName: \"kubernetes.io/empty-dir/8c3f8cd9-a3a9-4355-acc5-231869b60936-tmp-volume\") pod \"dashboard-metrics-scraper-5d59dccf9b-t6pxn\" (UID: \"8c3f8cd9-a3a9-4355-acc5-231869b60936\") " pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-t6pxn"
Aug 25 08:49:22 minikube kubelet[2656]: I0825 08:49:22.727302    2656 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-xc7jp\" (UniqueName: \"kubernetes.io/projected/e0bdc04f-13ed-4608-931a-eaaf17ae9bf9-kube-api-access-xc7jp\") pod \"kubernetes-dashboard-7779f9b69b-j2xvg\" (UID: \"e0bdc04f-13ed-4608-931a-eaaf17ae9bf9\") " pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-j2xvg"
Aug 25 08:49:22 minikube kubelet[2656]: I0825 08:49:22.727683    2656 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-volume\" (UniqueName: \"kubernetes.io/empty-dir/e0bdc04f-13ed-4608-931a-eaaf17ae9bf9-tmp-volume\") pod \"kubernetes-dashboard-7779f9b69b-j2xvg\" (UID: \"e0bdc04f-13ed-4608-931a-eaaf17ae9bf9\") " pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-j2xvg"
Aug 25 08:49:28 minikube kubelet[2656]: I0825 08:49:28.049684    2656 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3f21c13b01a8cc10907148204f777b6fdb0da37e889ea2d7914eb5b3ae2f6293"
Aug 25 08:50:12 minikube kubelet[2656]: I0825 08:50:12.314885    2656 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-t6pxn" podStartSLOduration=37.440465566 podStartE2EDuration="50.301412192s" podCreationTimestamp="2025-08-25 08:49:22 +0000 UTC" firstStartedPulling="2025-08-25 08:49:28.436031407 +0000 UTC m=+148.057040709" lastFinishedPulling="2025-08-25 08:49:41.296978133 +0000 UTC m=+160.917987335" observedRunningTime="2025-08-25 08:49:44.480942953 +0000 UTC m=+164.101952155" watchObservedRunningTime="2025-08-25 08:50:12.301412192 +0000 UTC m=+191.922850464"
Aug 25 08:50:12 minikube kubelet[2656]: I0825 08:50:12.315347    2656 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-j2xvg" podStartSLOduration=10.769213189 podStartE2EDuration="50.315320207s" podCreationTimestamp="2025-08-25 08:49:22 +0000 UTC" firstStartedPulling="2025-08-25 08:49:28.608909595 +0000 UTC m=+148.229918797" lastFinishedPulling="2025-08-25 08:50:08.154587443 +0000 UTC m=+187.776025815" observedRunningTime="2025-08-25 08:50:12.300917352 +0000 UTC m=+191.922355624" watchObservedRunningTime="2025-08-25 08:50:12.315320207 +0000 UTC m=+191.936758479"
Aug 25 09:00:37 minikube kubelet[2656]: I0825 09:00:37.346850    2656 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vb69v\" (UniqueName: \"kubernetes.io/projected/5c515f01-df1d-4da4-bc11-b8d4fd9204b2-kube-api-access-vb69v\") pod \"hello-node-6ff47567bc-6gn2m\" (UID: \"5c515f01-df1d-4da4-bc11-b8d4fd9204b2\") " pod="default/hello-node-6ff47567bc-6gn2m"
Aug 25 09:00:40 minikube kubelet[2656]: I0825 09:00:40.235490    2656 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="96fc5dff869a9bb67d0ffdef369ca8e78a11a05a654f1a3ef851f1ed5e8b5b30"
Aug 25 09:00:44 minikube kubelet[2656]: E0825 09:00:44.748990    2656 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hospital:1.0"
Aug 25 09:00:44 minikube kubelet[2656]: E0825 09:00:44.796044    2656 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hospital:1.0"
Aug 25 09:00:44 minikube kubelet[2656]: E0825 09:00:44.962733    2656 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:hospital,Image:hospital:1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vb69v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-node-6ff47567bc-6gn2m_default(5c515f01-df1d-4da4-bc11-b8d4fd9204b2): ErrImagePull: Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Aug 25 09:00:45 minikube kubelet[2656]: E0825 09:00:44.965307    2656 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hospital\" with ErrImagePull: \"Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-node-6ff47567bc-6gn2m" podUID="5c515f01-df1d-4da4-bc11-b8d4fd9204b2"
Aug 25 09:00:45 minikube kubelet[2656]: E0825 09:00:45.300153    2656 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hospital\" with ImagePullBackOff: \"Back-off pulling image \\\"hospital:1.0\\\": ErrImagePull: Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-node-6ff47567bc-6gn2m" podUID="5c515f01-df1d-4da4-bc11-b8d4fd9204b2"
Aug 25 09:01:03 minikube kubelet[2656]: E0825 09:01:03.395373    2656 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hospital:1.0"
Aug 25 09:01:03 minikube kubelet[2656]: E0825 09:01:03.395632    2656 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hospital:1.0"
Aug 25 09:01:03 minikube kubelet[2656]: E0825 09:01:03.396020    2656 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:hospital,Image:hospital:1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vb69v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-node-6ff47567bc-6gn2m_default(5c515f01-df1d-4da4-bc11-b8d4fd9204b2): ErrImagePull: Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Aug 25 09:01:03 minikube kubelet[2656]: E0825 09:01:03.397366    2656 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hospital\" with ErrImagePull: \"Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-node-6ff47567bc-6gn2m" podUID="5c515f01-df1d-4da4-bc11-b8d4fd9204b2"
Aug 25 09:01:16 minikube kubelet[2656]: E0825 09:01:16.628631    2656 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hospital\" with ImagePullBackOff: \"Back-off pulling image \\\"hospital:1.0\\\": ErrImagePull: Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-node-6ff47567bc-6gn2m" podUID="5c515f01-df1d-4da4-bc11-b8d4fd9204b2"
Aug 25 09:01:34 minikube kubelet[2656]: E0825 09:01:34.740680    2656 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hospital:1.0"
Aug 25 09:01:34 minikube kubelet[2656]: E0825 09:01:34.740760    2656 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hospital:1.0"
Aug 25 09:01:34 minikube kubelet[2656]: E0825 09:01:34.740889    2656 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:hospital,Image:hospital:1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vb69v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-node-6ff47567bc-6gn2m_default(5c515f01-df1d-4da4-bc11-b8d4fd9204b2): ErrImagePull: Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Aug 25 09:01:34 minikube kubelet[2656]: E0825 09:01:34.742177    2656 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hospital\" with ErrImagePull: \"Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-node-6ff47567bc-6gn2m" podUID="5c515f01-df1d-4da4-bc11-b8d4fd9204b2"
Aug 25 09:01:49 minikube kubelet[2656]: E0825 09:01:49.619144    2656 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hospital\" with ImagePullBackOff: \"Back-off pulling image \\\"hospital:1.0\\\": ErrImagePull: Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-node-6ff47567bc-6gn2m" podUID="5c515f01-df1d-4da4-bc11-b8d4fd9204b2"
Aug 25 09:02:00 minikube kubelet[2656]: E0825 09:02:00.620517    2656 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hospital\" with ImagePullBackOff: \"Back-off pulling image \\\"hospital:1.0\\\": ErrImagePull: Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-node-6ff47567bc-6gn2m" podUID="5c515f01-df1d-4da4-bc11-b8d4fd9204b2"
Aug 25 09:02:11 minikube kubelet[2656]: E0825 09:02:11.622921    2656 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hospital\" with ImagePullBackOff: \"Back-off pulling image \\\"hospital:1.0\\\": ErrImagePull: Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-node-6ff47567bc-6gn2m" podUID="5c515f01-df1d-4da4-bc11-b8d4fd9204b2"
Aug 25 09:02:27 minikube kubelet[2656]: E0825 09:02:27.530293    2656 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hospital:1.0"
Aug 25 09:02:27 minikube kubelet[2656]: E0825 09:02:27.530355    2656 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hospital:1.0"
Aug 25 09:02:27 minikube kubelet[2656]: E0825 09:02:27.530481    2656 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:hospital,Image:hospital:1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vb69v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-node-6ff47567bc-6gn2m_default(5c515f01-df1d-4da4-bc11-b8d4fd9204b2): ErrImagePull: Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Aug 25 09:02:27 minikube kubelet[2656]: E0825 09:02:27.531754    2656 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hospital\" with ErrImagePull: \"Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-node-6ff47567bc-6gn2m" podUID="5c515f01-df1d-4da4-bc11-b8d4fd9204b2"
Aug 25 09:02:43 minikube kubelet[2656]: E0825 09:02:43.673766    2656 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hospital\" with ImagePullBackOff: \"Back-off pulling image \\\"hospital:1.0\\\": ErrImagePull: Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-node-6ff47567bc-6gn2m" podUID="5c515f01-df1d-4da4-bc11-b8d4fd9204b2"
Aug 25 09:02:55 minikube kubelet[2656]: E0825 09:02:55.616524    2656 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hospital\" with ImagePullBackOff: \"Back-off pulling image \\\"hospital:1.0\\\": ErrImagePull: Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-node-6ff47567bc-6gn2m" podUID="5c515f01-df1d-4da4-bc11-b8d4fd9204b2"
Aug 25 09:03:09 minikube kubelet[2656]: E0825 09:03:09.623130    2656 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hospital\" with ImagePullBackOff: \"Back-off pulling image \\\"hospital:1.0\\\": ErrImagePull: Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-node-6ff47567bc-6gn2m" podUID="5c515f01-df1d-4da4-bc11-b8d4fd9204b2"
Aug 25 09:03:23 minikube kubelet[2656]: E0825 09:03:23.621746    2656 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hospital\" with ImagePullBackOff: \"Back-off pulling image \\\"hospital:1.0\\\": ErrImagePull: Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-node-6ff47567bc-6gn2m" podUID="5c515f01-df1d-4da4-bc11-b8d4fd9204b2"
Aug 25 09:03:35 minikube kubelet[2656]: E0825 09:03:35.619268    2656 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hospital\" with ImagePullBackOff: \"Back-off pulling image \\\"hospital:1.0\\\": ErrImagePull: Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-node-6ff47567bc-6gn2m" podUID="5c515f01-df1d-4da4-bc11-b8d4fd9204b2"
Aug 25 09:03:52 minikube kubelet[2656]: E0825 09:03:52.826190    2656 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hospital:1.0"
Aug 25 09:03:52 minikube kubelet[2656]: E0825 09:03:52.826326    2656 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hospital:1.0"
Aug 25 09:03:52 minikube kubelet[2656]: E0825 09:03:52.826726    2656 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:hospital,Image:hospital:1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vb69v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-node-6ff47567bc-6gn2m_default(5c515f01-df1d-4da4-bc11-b8d4fd9204b2): ErrImagePull: Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Aug 25 09:03:52 minikube kubelet[2656]: E0825 09:03:52.828294    2656 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hospital\" with ErrImagePull: \"Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-node-6ff47567bc-6gn2m" podUID="5c515f01-df1d-4da4-bc11-b8d4fd9204b2"
Aug 25 09:04:07 minikube kubelet[2656]: E0825 09:04:07.613773    2656 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hospital\" with ImagePullBackOff: \"Back-off pulling image \\\"hospital:1.0\\\": ErrImagePull: Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-node-6ff47567bc-6gn2m" podUID="5c515f01-df1d-4da4-bc11-b8d4fd9204b2"
Aug 25 09:04:20 minikube kubelet[2656]: E0825 09:04:20.619588    2656 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hospital\" with ImagePullBackOff: \"Back-off pulling image \\\"hospital:1.0\\\": ErrImagePull: Error response from daemon: pull access denied for hospital, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-node-6ff47567bc-6gn2m" podUID="5c515f01-df1d-4da4-bc11-b8d4fd9204b2"


==> kubernetes-dashboard [35a768e65284] <==
2025/08/25 08:52:08 [2025-08-25T08:52:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:09 [2025-08-25T08:52:09Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/08/25 08:52:09 Getting list of namespaces
2025/08/25 08:52:09 [2025-08-25T08:52:09Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:10 [2025-08-25T08:52:10Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/08/25 08:52:10 Getting list of all deployments in the cluster
2025/08/25 08:52:10 [2025-08-25T08:52:10Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:15 [2025-08-25T08:52:15Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2025/08/25 08:52:15 [2025-08-25T08:52:15Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:15 [2025-08-25T08:52:15Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/08/25 08:52:15 Getting list of all deployments in the cluster
2025/08/25 08:52:15 [2025-08-25T08:52:15Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/08/25 08:52:15 Getting list of namespaces
2025/08/25 08:52:15 [2025-08-25T08:52:15Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:15 [2025-08-25T08:52:15Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:16 [2025-08-25T08:52:16Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/08/25 08:52:17 [2025-08-25T08:52:17Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:20 [2025-08-25T08:52:20Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/08/25 08:52:20 Getting list of namespaces
2025/08/25 08:52:20 [2025-08-25T08:52:20Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:22 [2025-08-25T08:52:22Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/08/25 08:52:22 [2025-08-25T08:52:22Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:22 [2025-08-25T08:52:22Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2025/08/25 08:52:22 [2025-08-25T08:52:22Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:24 [2025-08-25T08:52:24Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/08/25 08:52:24 Getting list of all cron jobs in the cluster
2025/08/25 08:52:24 [2025-08-25T08:52:24Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:26 [2025-08-25T08:52:26Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/08/25 08:52:26 Getting list of namespaces
2025/08/25 08:52:26 [2025-08-25T08:52:26Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:28 [2025-08-25T08:52:28Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/08/25 08:52:28 Getting list of all cron jobs in the cluster
2025/08/25 08:52:28 [2025-08-25T08:52:28Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:30 [2025-08-25T08:52:30Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/08/25 08:52:30 Getting list of namespaces
2025/08/25 08:52:30 [2025-08-25T08:52:30Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:33 [2025-08-25T08:52:33Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/08/25 08:52:33 Getting list of all cron jobs in the cluster
2025/08/25 08:52:33 [2025-08-25T08:52:33Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:35 [2025-08-25T08:52:35Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/08/25 08:52:35 Getting list of namespaces
2025/08/25 08:52:35 [2025-08-25T08:52:35Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:36 [2025-08-25T08:52:36Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/08/25 08:52:36 Getting list of all cron jobs in the cluster
2025/08/25 08:52:36 [2025-08-25T08:52:36Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/08/25 08:52:36 Getting list of namespaces
2025/08/25 08:52:36 [2025-08-25T08:52:36Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:36 [2025-08-25T08:52:36Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:54 [2025-08-25T08:52:54Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/08/25 08:52:54 Getting list of namespaces
2025/08/25 08:52:54 [2025-08-25T08:52:54Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/08/25 08:52:54 Getting list of all cron jobs in the cluster
2025/08/25 08:52:54 [2025-08-25T08:52:54Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:54 [2025-08-25T08:52:54Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:57 [2025-08-25T08:52:57Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/08/25 08:52:57 Getting list of namespaces
2025/08/25 08:52:57 [2025-08-25T08:52:57Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/08/25 08:52:57 Getting list of all cron jobs in the cluster
2025/08/25 08:52:57 [2025-08-25T08:52:57Z] Outcoming response to 127.0.0.1 with 200 status code
2025/08/25 08:52:57 [2025-08-25T08:52:57Z] Outcoming response to 127.0.0.1 with 200 status code


==> storage-provisioner [842d7f7c7cd5] <==
I0825 08:47:36.028874       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0825 08:47:57.225683       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused


==> storage-provisioner [ef65bc3ff63a] <==
W0825 09:03:32.326597       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:32.401314       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:34.406596       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:34.445780       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:36.513318       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:36.545972       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:38.550534       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:38.597463       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:40.600656       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:40.652819       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:42.657301       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:42.676698       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:44.682105       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:44.824765       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:46.830325       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:46.870722       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:48.875080       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:48.913308       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:50.919739       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:50.962258       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:52.993527       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:53.029356       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:55.032406       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:55.097019       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:57.099827       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:57.141309       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:59.145637       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:03:59.258780       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:01.263832       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:01.332326       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:03.337096       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:03.414128       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:05.419045       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:05.475726       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:07.480737       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:07.514323       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:09.517602       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:09.546156       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:11.549831       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:11.602351       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:13.607323       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:13.652970       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:15.656595       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:15.739038       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:17.740757       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:17.819106       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:19.831894       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:19.877654       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:21.880167       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:21.977051       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:23.980156       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:24.201955       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:26.207218       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:26.255009       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:28.258313       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:28.319623       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:30.324105       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:30.479483       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:32.486062       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0825 09:04:32.615691       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

